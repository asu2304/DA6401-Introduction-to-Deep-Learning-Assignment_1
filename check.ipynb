{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e46e56-b9f2-4ac2-992f-0975c6cbaed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learning_rate': 0.001, 'epochs': 10, 'batch_size': 32, 'layer_size': [784, 128, 64, 10], 'optimizer': 'Rms_prop', 'beta_1': 0.9, 'beta_2': 0.999, 'loss_function': 'categorical_cross_entropy', 'weight_decay': 0, 'activation_function': 'Sigmoid', 'initialization': 'He'}\n",
      "epoch: 0, train_loss:0.6159, val_accuracy: 0.8391\n",
      "epoch: 1, train_loss:0.4048, val_accuracy: 0.8613\n",
      "epoch: 2, train_loss:0.3670, val_accuracy: 0.8539\n",
      "epoch: 3, train_loss:0.3443, val_accuracy: 0.8615\n",
      "epoch: 4, train_loss:0.3278, val_accuracy: 0.8762\n",
      "epoch: 5, train_loss:0.3140, val_accuracy: 0.8784\n",
      "epoch: 6, train_loss:0.3021, val_accuracy: 0.8562\n",
      "epoch: 7, train_loss:0.2913, val_accuracy: 0.8806\n",
      "epoch: 8, train_loss:0.2816, val_accuracy: 0.8830\n",
      "epoch: 9, train_loss:0.2732, val_accuracy: 0.8895\n",
      "test_accuracy: 0.8809\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "\n",
    "class backprop_from_scratch:\n",
    "    \n",
    "    def __init__(self, layer_size, activation_function='sigmoid'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the neural network with the specified layer sizes and activation function.\n",
    "        \n",
    "        Args:\n",
    "            layer_size (list): List of integers specifying the size of each layer.\n",
    "            activation_function (str): Activation function for hidden layers ('sigmoid', 'relu', 'tanh', 'linear').\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate activation function\n",
    "        valid_activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
    "        if activation_function.lower() not in valid_activations:\n",
    "            raise ValueError(f\"Activation function must be one of {valid_activations}, got {activation_function}\")\n",
    "        self.activation_function = activation_function.lower()\n",
    "        \n",
    "        # Initialize network parameters\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.num_layers = len(layer_size)\n",
    "        self.layer_sizes = layer_size\n",
    "        self.initialize_params()\n",
    "        \n",
    "        # Initialize histories and velocities for optimizers\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.history_bias = [np.zeros_like(b) for b in self.biases]\n",
    "        self.weights_velocity = [np.zeros_like(w) for w in self.weights]\n",
    "        self.bias_velocity = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Default beta values for optimizers\n",
    "        self.beta_1 = 0.900\n",
    "        self.beta_2 = 0.999\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"Initialize weights with He initialization and biases with zeros.\"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            b = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def apply_activation(self, a):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function to the input.\n",
    "        \n",
    "        Args:\n",
    "            a (np.ndarray): Input array to apply the activation function to.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Activated output.\n",
    "        \"\"\"\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(a)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return self.relu(a)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh(a)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear(a)\n",
    "\n",
    "    def get_activation_derivative(self, h):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function based on the output.\n",
    "        \n",
    "        Args:\n",
    "            h (np.ndarray): Output of the activation function.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return h * (1 - h)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return (h > 0).astype(float)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return 1 - h ** 2\n",
    "        elif self.activation_function == 'linear':\n",
    "            return np.ones_like(h)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function, vectorized.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function, vectorized.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function, vectorized.\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear activation function (identity), vectorized.\"\"\"\n",
    "        return x\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for the output layer, vectorized.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward_pass(self, data_x, optimizer='rms_prop'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            data_x (np.ndarray): Input data.\n",
    "            optimizer (str): Optimizer type to determine weight updates.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of neuron outputs at each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        neuron_outputs = [data_x]\n",
    "        if optimizer.lower() != 'nag':\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "                h = self.apply_activation(a)\n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # Output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1]) + self.biases[-1]\n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "            \n",
    "        else:  # NAG optimizer\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = (np.dot(neuron_outputs[-1], self.weights[i] - self.beta_1 * self.weights_velocity[i]) +\n",
    "                     self.biases[i] - self.beta_1 * self.bias_velocity[i])\n",
    "                h = self.apply_activation(a)\n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # Output layer\n",
    "            a = (np.dot(neuron_outputs[-1], self.weights[-1] - self.beta_1 * self.weights_velocity[-1]) +\n",
    "                 self.biases[-1] - self.beta_1 * self.bias_velocity[-1])\n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "        return neuron_outputs\n",
    "\n",
    "    def backward_pass(self, x, y, neuron_outputs, learning_rate, t, optimizer, loss_function, weight_decay):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to update weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            x (np.ndarray): Input batch.\n",
    "            y (np.ndarray): True labels.\n",
    "            neuron_outputs (list): Outputs from the forward pass.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            t (int): Current iteration for bias correction in optimizers.\n",
    "            optimizer (str): Optimizer type.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): Weight decay parameter.\n",
    "        \"\"\"\n",
    "        batch_size = len(x)\n",
    "        # Compute initial delta for output layer\n",
    "        if loss_function.lower() == 'categorical_cross_entropy':\n",
    "            delta = neuron_outputs[-1] - y\n",
    "        else:  # Mean squared error\n",
    "            delta = np.zeros((batch_size, self.layer_sizes[-1]))\n",
    "            for i in range(batch_size):\n",
    "                softmax_jacobian = (np.diag(neuron_outputs[-1][i]) -\n",
    "                                   np.outer(neuron_outputs[-1][i], neuron_outputs[-1][i]))\n",
    "                delta[i] = 2 * np.dot(neuron_outputs[-1][i] - y[i], softmax_jacobian)\n",
    "\n",
    "        # Update weights and biases layer by layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                \n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                \n",
    "                if i > 0:\n",
    "                    delta = np.dot(delta, self.weights[i].T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "\n",
    "                if optimizer.lower() == 'sgd':\n",
    "                    self.weights[i] -= learning_rate * dw\n",
    "                    self.biases[i] -= learning_rate * db\n",
    "                    \n",
    "                elif optimizer.lower() == 'momentum sgd':\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                    self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                    self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "                    \n",
    "                elif optimizer.lower() == 'rms_prop':\n",
    "                    self.history_weights[i] = (self.beta_1 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_1) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_1 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_1) * (db ** 2))\n",
    "                    self.weights[i] -= learning_rate * dw / np.sqrt(self.history_weights[i] + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * db / np.sqrt(self.history_bias[i] + 1e-12)\n",
    "                    \n",
    "                elif optimizer.lower() == 'adam':\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    w_vel_corr = self.weights_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    b_vel_corr = self.bias_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    self.weights[i] -= learning_rate * w_vel_corr / np.sqrt(w_hist_corr + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * b_vel_corr / np.sqrt(b_hist_corr + 1e-12)\n",
    "                    \n",
    "                elif optimizer.lower() == 'nadam':\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    w_momentum = (self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw) / (1 - self.beta_1 ** t)\n",
    "                    b_momentum = (self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db) / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    self.weights[i] -= learning_rate * w_momentum / np.sqrt(w_hist_corr + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * b_momentum / np.sqrt(b_hist_corr + 1e-12)\n",
    "                    \n",
    "        else:  # NAG optimizer\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:\n",
    "                    delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i]).T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "                self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=50, learning_rate=0.001, batch_size=32,\n",
    "              optimizer=\"nag\", beta_1=0.900, beta_2=0.999, loss_function='categorical_cross_entropy',\n",
    "              weight_decay=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "            x_train (np.ndarray): Training input data.\n",
    "            y_train (np.ndarray): Training labels.\n",
    "            x_val (np.ndarray): Validation input data.\n",
    "            y_val (np.ndarray): Validation labels.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate.\n",
    "            batch_size (int): Size of each mini-batch.\n",
    "            optimizer (str): Optimizer type.\n",
    "            beta_1 (float): Momentum parameter.\n",
    "            beta_2 (float): RMSProp/Adam parameter.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): Weight decay parameter.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(x_train.shape[0])\n",
    "            \n",
    "            x_train_permuted = x_train[indices]\n",
    "            y_train_permuted = y_train[indices]\n",
    "            total_loss = 0\n",
    "            batch_num = 0\n",
    "            \n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                \n",
    "                batch_x = x_train_permuted[i:i + batch_size]\n",
    "                batch_y = y_train_permuted[i:i + batch_size]\n",
    "                neuron_outputs = self.forward_pass(batch_x, optimizer)\n",
    "                l2_norm_params = sum(np.sum(w ** 2) for w in self.weights) + sum(np.sum(b ** 2) for b in self.biases)\n",
    "                if loss_function.lower() == 'categorical_cross_entropy':\n",
    "                    loss = (-np.mean(np.sum(batch_y * np.log(neuron_outputs[-1] + 1e-10), axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                else:\n",
    "                    loss = (np.mean(np.sum((batch_y - neuron_outputs[-1]) ** 2, axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                total_loss += loss\n",
    "                batch_num += 1\n",
    "                self.backward_pass(batch_x, batch_y, neuron_outputs, learning_rate, batch_num, optimizer,\n",
    "                                  loss_function, weight_decay)\n",
    "                \n",
    "            average_loss = total_loss / batch_num\n",
    "            validation_predictions = self.predict(x_val)\n",
    "            validation_accuracy = np.mean(validation_predictions == np.argmax(y_val, axis=1))\n",
    "            wandb.log({'epoch': epoch, 'train_loss': average_loss, 'val_accuracy': validation_accuracy})\n",
    "            print(f\"epoch: {epoch}, train_loss:{average_loss:.4f}, val_accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for the input data.\n",
    "        \n",
    "        Args:\n",
    "            x (np.ndarray): Input data.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Predicted class indices.\n",
    "        \"\"\"\n",
    "        neuron_outputs = self.forward_pass(x)\n",
    "        return np.argmax(neuron_outputs[-1], axis=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    wandb.login()\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    # Prepare data\n",
    "    indices = np.arange(train_images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    train_size = 50000\n",
    "    \n",
    "    train_x = train_images[indices[:train_size]].reshape(-1, 784) / 255\n",
    "    train_y = np.eye(10)[train_labels[indices[:train_size]]]\n",
    "    val_x = train_images[indices[train_size:]].reshape(-1, 784) / 255\n",
    "    val_y = np.eye(10)[train_labels[indices[train_size:]]]\n",
    "    \n",
    "    test_images = test_images.reshape(-1, 784) / 255\n",
    "    test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(project='backprop_scratch', config={\n",
    "        'Learning_rate': 0.001,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'layer_size': [784, 128, 64, 10],\n",
    "        'optimizer': 'rms_prop',\n",
    "        'beta_1': 0.900,\n",
    "        'beta_2': 0.999,\n",
    "        'loss_function': 'categorical_cross_entropy',\n",
    "        'weight_decay': 0,\n",
    "        'activation_function': 'relu'\n",
    "    })\n",
    "    \n",
    "    config = wandb.config\n",
    "    print(wandb.config)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = backprop_from_scratch(config.layer_size, config.activation_function)\n",
    "    model.train(train_x, train_y, val_x, val_y, config.epochs, config.Learning_rate, config.batch_size,\n",
    "                config.optimizer, config.beta_1, config.beta_2, config.loss_function, config.weight_decay)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_predictions = model.predict(test_images)\n",
    "    test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))\n",
    "    print(f\"test_accuracy: {test_accuracy:.4f}\")\n",
    "    wandb.log({'test_accuracy': test_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad5fb9-3b36-4527-b5d8-e89cb2478a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175f90a7-f708-4bb8-85cf-efd3a01165a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset {mnist,fashion_mnist}] [--hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]]\n",
      "                             [--activation_function {sigmoid,relu,tanh,linear}] [--learning_rate LEARNING_RATE]\n",
      "                             [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--optimizer {sgd,momentum sgd,nag,rms_prop,adam,nadam}] [--beta_1 BETA_1]\n",
      "                             [--beta_2 BETA_2] [--loss_function {categorical_cross_entropy,mean_squared_error}]\n",
      "                             [--weight_decay WEIGHT_DECAY] [--epsilon EPSILON]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Ashutosh Patidar\\AppData\\Roaming\\jupyter\\runtime\\kernel-81958830-bda1-409d-85de-4425b3321a1b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.datasets import mnist  # Added to support MNIST dataset\n",
    "from keras.datasets import fashion_mnist  # Original dataset import\n",
    "\n",
    "import wandb  # For logging and sweep functionality\n",
    "import argparse  # For parsing command-line arguments\n",
    "\n",
    "# Define the neural network class\n",
    "class backprop_from_scratch:\n",
    "    \n",
    "    def __init__(self, layer_size, activation_function='sigmoid', epsilon=1e-8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the neural network with layer sizes, activation function, and epsilon.\n",
    "\n",
    "        Args:\n",
    "            layer_size (list): List of integers specifying the size of each layer (input, hidden, output).\n",
    "            activation_function (str): Activation function for hidden layers ('sigmoid', 'relu', 'tanh', 'linear').\n",
    "            epsilon (float): Small value for numerical stability in computations (e.g., avoiding log(0)).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate activation function to ensure it’s supported\n",
    "        valid_activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
    "        if activation_function.lower() not in valid_activations:\n",
    "            raise ValueError(f\"Activation function must be one of {valid_activations}, got {activation_function}\")\n",
    "        self.activation_function = activation_function.lower()  # Store as lowercase for consistency\n",
    "        self.epsilon = epsilon  # Store epsilon for use in loss and optimizer calculations\n",
    "        \n",
    "        # Initialize network architecture and parameters\n",
    "        self.weights = []  # List to hold weight matrices\n",
    "        self.biases = []  # List to hold bias vectors\n",
    "        self.num_layers = len(layer_size)  # Total number of layers (input + hidden + output)\n",
    "        self.layer_sizes = layer_size  # Store layer sizes for reference\n",
    "        self.initialize_params()  # Initialize weights and biases\n",
    "        \n",
    "        # Initialize optimizer histories and velocities with zeros, matching weight/bias shapes\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.weights]  # For RMSProp/Adam history\n",
    "        self.history_bias = [np.zeros_like(b) for b in self.biases]  # For RMSProp/Adam history\n",
    "        self.weights_velocity = [np.zeros_like(w) for w in self.weights]  # For momentum/NAG\n",
    "        self.bias_velocity = [np.zeros_like(b) for b in self.biases]  # For momentum/NAG\n",
    "        \n",
    "        # Default beta values for optimizers (will be overridden if provided)\n",
    "        self.beta_1 = 0.900  # Momentum/RMSProp/Adam beta1\n",
    "        self.beta_2 = 0.999  # Adam/Nadam beta2\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"Initialize weights with He initialization and biases with zeros.\"\"\"\n",
    "        # Loop through layers to create weight matrices and bias vectors\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # He initialization for weights: scales by sqrt(2/input_size) for ReLU compatibility\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            b = np.zeros((1, self.layer_sizes[i + 1]))  # Biases initialized to zero\n",
    "            self.weights.append(w)  # Add weight matrix to list\n",
    "            self.biases.append(b)  # Add bias vector to list\n",
    "\n",
    "    def apply_activation(self, a):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function to the input.\n",
    "\n",
    "        Args:\n",
    "            a (np.ndarray): Input array (pre-activation values).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Activated output.\n",
    "        \"\"\"\n",
    "        # Select activation function based on self.activation_function\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(a)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return self.relu(a)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh(a)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear(a)\n",
    "\n",
    "    def get_activation_derivative(self, h):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function based on its output.\n",
    "\n",
    "        Args:\n",
    "            h (np.ndarray): Output of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        # Compute derivative based on activation function type\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return h * (1 - h)  # Sigmoid derivative: h * (1 - h)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return (h > 0).astype(float)  # ReLU derivative: 1 if h > 0, else 0\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return 1 - h ** 2  # Tanh derivative: 1 - h^2\n",
    "        elif self.activation_function == 'linear':\n",
    "            return np.ones_like(h)  # Linear derivative: constant 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function with clipping to prevent overflow.\"\"\"\n",
    "        # Clip input to [-500, 500] to avoid exp overflow, then compute sigmoid\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)  # Return max(0, x) element-wise\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(x)  # NumPy’s built-in tanh function\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear (identity) activation function.\"\"\"\n",
    "        return x  # Simply return the input unchanged\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for the output layer.\"\"\"\n",
    "        # Subtract max per row for numerical stability, then compute softmax\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "\n",
    "    def forward_pass(self, data_x, optimizer='rms_prop'):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            data_x (np.ndarray): Input data (batch_size x input_size).\n",
    "            optimizer (str): Optimizer type, affects NAG computation.\n",
    "\n",
    "        Returns:\n",
    "            list: Neuron outputs at each layer (including input and output).\n",
    "        \"\"\"\n",
    "        neuron_outputs = [data_x]  # Start with input layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            # Standard forward pass for non-NAG optimizers\n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i]) + self.biases[i]  # Linear transformation\n",
    "                h = self.apply_activation(a)  # Apply activation\n",
    "                neuron_outputs.append(h)  # Store output\n",
    "            # Output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1]) + self.biases[-1]  # Linear transformation\n",
    "            output = self.softmax(a)  # Apply softmax for probabilities\n",
    "            neuron_outputs.append(output)  # Store output\n",
    "        else:\n",
    "            # NAG forward pass: look-ahead using velocity\n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                # Adjust weights and biases with momentum terms\n",
    "                a = (np.dot(neuron_outputs[-1], self.weights[i] - self.beta_1 * self.weights_velocity[i]) +\n",
    "                     self.biases[i] - self.beta_1 * self.bias_velocity[i])\n",
    "                h = self.apply_activation(a)  # Apply activation\n",
    "                neuron_outputs.append(h)  # Store output\n",
    "            # Output layer\n",
    "            a = (np.dot(neuron_outputs[-1], self.weights[-1] - self.beta_1 * self.weights_velocity[-1]) +\n",
    "                 self.biases[-1] - self.beta_1 * self.bias_velocity[-1])\n",
    "            output = self.softmax(a)  # Apply softmax\n",
    "            neuron_outputs.append(output)  # Store output\n",
    "        return neuron_outputs\n",
    "\n",
    "    def backward_pass(self, x, y, neuron_outputs, learning_rate, t, optimizer, loss_function, weight_decay):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to update weights and biases.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input batch.\n",
    "            y (np.ndarray): True labels (one-hot encoded).\n",
    "            neuron_outputs (list): Outputs from forward pass.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            t (int): Current iteration (for bias correction in Adam/Nadam).\n",
    "            optimizer (str): Optimizer type (sgd, momentum sgd, nag, rms_prop, adam, nadam).\n",
    "            loss_function (str): Loss function (categorical_cross_entropy or mean_squared_error).\n",
    "            weight_decay (float): L2 regularization parameter.\n",
    "        \"\"\"\n",
    "        batch_size = len(x)  # Number of samples in the batch\n",
    "        # Compute initial delta for output layer\n",
    "        if loss_function.lower() == 'categorical_cross_entropy':\n",
    "            delta = neuron_outputs[-1] - y  # Gradient of cross-entropy w.r.t. softmax output\n",
    "        else:  # Mean squared error\n",
    "            delta = np.zeros((batch_size, self.layer_sizes[-1]))  # Initialize delta array\n",
    "            for i in range(batch_size):\n",
    "                # Compute softmax Jacobian for MSE gradient\n",
    "                softmax_jacobian = (np.diag(neuron_outputs[-1][i]) -\n",
    "                                   np.outer(neuron_outputs[-1][i], neuron_outputs[-1][i]))\n",
    "                delta[i] = 2 * np.dot(neuron_outputs[-1][i] - y[i], softmax_jacobian)  # MSE gradient\n",
    "\n",
    "        # Update weights and biases from output to input layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            for i in range(self.num_layers - 2, -1, -1):  # Iterate backwards\n",
    "                # Compute gradients with weight decay\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:  # If not the input layer\n",
    "                    # Propagate delta backwards\n",
    "                    delta = np.dot(delta, self.weights[i].T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "\n",
    "                # Update parameters based on optimizer\n",
    "                if optimizer.lower() == 'sgd':\n",
    "                    self.weights[i] -= learning_rate * dw  # Simple gradient descent\n",
    "                    self.biases[i] -= learning_rate * db\n",
    "                elif optimizer.lower() == 'momentum sgd':\n",
    "                    # Momentum updates\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                    self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                    self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "                elif optimizer.lower() == 'rms_prop':\n",
    "                    # RMSProp: update history with moving average of squared gradients\n",
    "                    self.history_weights[i] = (self.beta_1 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_1) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_1 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_1) * (db ** 2))\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * dw / np.sqrt(self.history_weights[i] + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * db / np.sqrt(self.history_bias[i] + self.epsilon)\n",
    "                elif optimizer.lower() == 'adam':\n",
    "                    # Adam: update velocity (first moment) and history (second moment)\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    # Bias-corrected estimates\n",
    "                    w_vel_corr = self.weights_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    b_vel_corr = self.bias_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * w_vel_corr / np.sqrt(w_hist_corr + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * b_vel_corr / np.sqrt(b_hist_corr + self.epsilon)\n",
    "                elif optimizer.lower() == 'nadam':\n",
    "                    # Nadam: Nesterov + Adam\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    # Nesterov momentum with bias correction\n",
    "                    w_momentum = (self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw) / (1 - self.beta_1 ** t)\n",
    "                    b_momentum = (self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db) / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * w_momentum / np.sqrt(w_hist_corr + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * b_momentum / np.sqrt(b_hist_corr + self.epsilon)\n",
    "        else:  # NAG optimizer\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                # Compute gradients with weight decay\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:\n",
    "                    # Propagate delta with NAG-adjusted weights\n",
    "                    delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i]).T) * \\\n",
    "                            self.get_activation_derivative(neuron_outputs[i])\n",
    "                # Update velocities and parameters\n",
    "                self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=50, learning_rate=0.001, batch_size=32,\n",
    "              optimizer=\"nag\", beta_1=0.900, beta_2=0.999, loss_function='categorical_cross_entropy',\n",
    "              weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network with mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            x_train (np.ndarray): Training input data.\n",
    "            y_train (np.ndarray): Training labels (one-hot encoded).\n",
    "            x_val (np.ndarray): Validation input data.\n",
    "            y_val (np.ndarray): Validation labels (one-hot encoded).\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            batch_size (int): Size of each mini-batch.\n",
    "            optimizer (str): Optimizer type.\n",
    "            beta_1 (float): Momentum/RMSProp/Adam parameter.\n",
    "            beta_2 (float): Adam/Nadam parameter.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): L2 regularization parameter.\n",
    "        \"\"\"\n",
    "        # Set optimizer parameters\n",
    "        self.beta_1 = beta_1  # Update instance beta_1\n",
    "        self.beta_2 = beta_2  # Update instance beta_2\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data for each epoch\n",
    "            indices = np.random.permutation(x_train.shape[0])\n",
    "            x_train_permuted = x_train[indices]\n",
    "            y_train_permuted = y_train[indices]\n",
    "            total_loss = 0  # Accumulate loss over batches\n",
    "            batch_num = 0  # Count batches for averaging\n",
    "            # Mini-batch training\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                batch_x = x_train_permuted[i:i + batch_size]  # Extract batch input\n",
    "                batch_y = y_train_permuted[i:i + batch_size]  # Extract batch labels\n",
    "                neuron_outputs = self.forward_pass(batch_x, optimizer)  # Forward pass\n",
    "                # Compute L2 regularization term\n",
    "                l2_norm_params = sum(np.sum(w ** 2) for w in self.weights) + sum(np.sum(b ** 2) for b in self.biases)\n",
    "                # Compute loss with epsilon for stability\n",
    "                if loss_function.lower() == 'categorical_cross_entropy':\n",
    "                    loss = (-np.mean(np.sum(batch_y * np.log(neuron_outputs[-1] + self.epsilon), axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                else:  # Mean squared error\n",
    "                    loss = (np.mean(np.sum((batch_y - neuron_outputs[-1]) ** 2, axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                total_loss += loss  # Add to total loss\n",
    "                batch_num += 1  # Increment batch counter\n",
    "                # Backward pass to update weights\n",
    "                self.backward_pass(batch_x, batch_y, neuron_outputs, learning_rate, batch_num, optimizer,\n",
    "                                  loss_function, weight_decay)\n",
    "            # Compute average loss for the epoch\n",
    "            average_loss = total_loss / batch_num\n",
    "            # Evaluate on validation set\n",
    "            validation_predictions = self.predict(x_val)\n",
    "            validation_accuracy = np.mean(validation_predictions == np.argmax(y_val, axis=1))\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({'epoch': epoch, 'train_loss': average_loss, 'val_accuracy': validation_accuracy})\n",
    "            # Print progress\n",
    "            print(f\"epoch: {epoch}, train_loss: {average_loss:.4f}, val_accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted class indices.\n",
    "        \"\"\"\n",
    "        neuron_outputs = self.forward_pass(x)  # Forward pass\n",
    "        return np.argmax(neuron_outputs[-1], axis=1)  # Return class with highest probability\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up command-line argument parser\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network with backpropagation from scratch.')\n",
    "    \n",
    "    # Dataset choice: mnist or fashion_mnist\n",
    "    parser.add_argument('--dataset', type=str, default='fashion_mnist', choices=['mnist', 'fashion_mnist'],\n",
    "                        help='Dataset to use (mnist or fashion_mnist)')\n",
    "    # Hidden layer sizes as a list (e.g., --hidden_sizes 128 64 for two layers)\n",
    "    parser.add_argument('--hidden_sizes', type=int, nargs='+', default=[128, 64],\n",
    "                        help='List of hidden layer sizes (e.g., 128 64)')\n",
    "    # Activation function choice\n",
    "    parser.add_argument('--activation_function', type=str, default='relu',\n",
    "                        choices=['sigmoid', 'relu', 'tanh', 'linear'],\n",
    "                        help='Activation function for hidden layers')\n",
    "    # Learning rate\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='Learning rate for optimization')\n",
    "    # Number of epochs\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help='Number of training epochs')\n",
    "    # Batch size\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Size of each mini-batch')\n",
    "    # Optimizer choice\n",
    "    parser.add_argument('--optimizer', type=str, default='rms_prop',\n",
    "                        choices=['sgd', 'momentum sgd', 'nag', 'rms_prop', 'adam', 'nadam'],\n",
    "                        help='Optimizer type')\n",
    "    # Beta1 for optimizers\n",
    "    parser.add_argument('--beta_1', type=float, default=0.900,\n",
    "                        help='Beta1 parameter for momentum/RMSProp/Adam')\n",
    "    # Beta2 for Adam/Nadam\n",
    "    parser.add_argument('--beta_2', type=float, default=0.999,\n",
    "                        help='Beta2 parameter for Adam/Nadam')\n",
    "    # Loss function choice\n",
    "    parser.add_argument('--loss_function', type=str, default='categorical_cross_entropy',\n",
    "                        choices=['categorical_cross_entropy', 'mean_squared_error'],\n",
    "                        help='Loss function type')\n",
    "    # Weight decay (L2 regularization)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0,\n",
    "                        help='Weight decay (L2 regularization) parameter')\n",
    "    # Epsilon for numerical stability\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                        help='Epsilon value for numerical stability')\n",
    "    # Parse arguments from command line\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Convert parsed arguments to a dictionary for wandb config\n",
    "    config = vars(args)\n",
    "    # Initialize wandb with the config; for sweeps, wandb will override specified hyperparameters\n",
    "    wandb.init(project='backprop_scratch', config=config)\n",
    "\n",
    "    def train():\n",
    "        \"\"\"Train the model using hyperparameters from wandb.config.\"\"\"\n",
    "        config = wandb.config  # Access hyperparameters (from command line or sweep)\n",
    "        # Load dataset based on config.dataset\n",
    "        if config.dataset == 'mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        elif config.dataset == 'fashion_mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {config.dataset}\")\n",
    "\n",
    "        # Preprocess data: flatten and normalize images, one-hot encode labels\n",
    "        train_images = train_images.reshape(-1, 784) / 255.0  # Flatten to 784, normalize to [0, 1]\n",
    "        test_images = test_images.reshape(-1, 784) / 255.0  # Same for test set\n",
    "        train_labels = np.eye(10)[train_labels]  # One-hot encode training labels\n",
    "        test_labels = np.eye(10)[test_labels]  # One-hot encode test labels\n",
    "\n",
    "        # Split training data into train and validation sets (50000 train, 10000 val)\n",
    "        indices = np.arange(train_images.shape[0])\n",
    "        np.random.shuffle(indices)  # Randomize indices\n",
    "        train_size = 50000  # Fixed split as in original code\n",
    "        train_x = train_images[indices[:train_size]]  # Training input\n",
    "        train_y = train_labels[indices[:train_size]]  # Training labels\n",
    "        val_x = train_images[indices[train_size:]]  # Validation input\n",
    "        val_y = train_labels[indices[train_size:]]  # Validation labels\n",
    "\n",
    "        # Construct layer_size: input (784), hidden layers, output (10)\n",
    "        layer_size = [784] + config.hidden_sizes + [10]  # Flexible hidden layers from config\n",
    "        # Create model instance with specified parameters\n",
    "        model = backprop_from_scratch(layer_size, config.activation_function, config.epsilon)\n",
    "        # Train the model with all hyperparameters from config\n",
    "        model.train(train_x, train_y, val_x, val_y, config.epochs, config.learning_rate, config.batch_size,\n",
    "                    config.optimizer, config.beta_1, config.beta_2, config.loss_function, config.weight_decay)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_predictions = model.predict(test_images)  # Get predictions\n",
    "        test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))  # Compute accuracy\n",
    "        wandb.log({'test_accuracy': test_accuracy})  # Log test accuracy to wandb\n",
    "        print(f\"test_accuracy: {test_accuracy:.4f}\")  # Print test accuracy\n",
    "\n",
    "    # Execute training\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06dd19-aded-4b99-bbcd-9110b271e788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09635c-ad96-4abb-a2e4-5c71e6f9279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network with backpropagation from scratch.')\n",
    "\n",
    "    # Weights & Biases arguments\n",
    "    parser.add_argument('-wp', '--wandb_project', type=str, default='myprojectname', help='Project name for Weights & Biases')\n",
    "    parser.add_argument('-we', '--wandb_entity', type=str, default='myname', help='Wandb Entity for tracking experiments')\n",
    "    \n",
    "    # Dataset and training arguments\n",
    "    parser.add_argument('-d', '--dataset', type=str, default='fashion_mnist', choices=['mnist', 'fashion_mnist'], help='Dataset to use')\n",
    "    parser.add_argument('-e', '--epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=4, help='Batch size')\n",
    "    parser.add_argument('-l', '--loss', type=str, default='cross_entropy', choices=['mean_squared_error', 'cross_entropy'], help='Loss function')\n",
    "    parser.add_argument('-o', '--optimizer', type=str, default='sgd', choices=['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam'], help='Optimizer')\n",
    "    parser.add_argument('-lr', '--learning_rate', type=float, default=0.1, help='Learning rate')\n",
    "    \n",
    "    # Optimizer-specific parameters\n",
    "    parser.add_argument('-m', '--momentum', type=float, default=0.5, help='Momentum for momentum-based optimizers')\n",
    "    parser.add_argument('-beta', '--beta', type=float, default=0.5, help='Beta for rmsprop optimizer')\n",
    "    parser.add_argument('-beta1', '--beta1', type=float, default=0.5, help='Beta1 for adam and nadam')\n",
    "    parser.add_argument('-beta2', '--beta2', type=float, default=0.5, help='Beta2 for adam and nadam')\n",
    "    parser.add_argument('-eps', '--epsilon', type=float, default=0.000001, help='Epsilon for numerical stability')\n",
    "    parser.add_argument('-w_d', '--weight_decay', type=float, default=0.0, help='Weight decay (L2 regularization)')\n",
    "    \n",
    "    # Neural network architecture\n",
    "    parser.add_argument('-w_i', '--weight_init', type=str, default='random', choices=['random', 'Xavier'], help='Weight initialization method')\n",
    "    parser.add_argument('-nhl', '--num_layers', type=int, default=1, help='Number of hidden layers')\n",
    "    parser.add_argument('-sz', '--hidden_size', type=int, default=4, help='Number of neurons in a hidden layer')\n",
    "    parser.add_argument('-a', '--activation', type=str, default='sigmoid', choices=['identity', 'sigmoid', 'tanh', 'ReLU'], help='Activation function')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Convert parsed arguments to a dictionary\n",
    "    config = vars(args)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project=config['wandb_project'], entity=config['wandb_entity'], config=config)\n",
    "\n",
    "    def train():\n",
    "        config = wandb.config  # Access hyperparameters\n",
    "        \n",
    "        # Load dataset\n",
    "        if config.dataset == 'mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        else:\n",
    "            (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_images = train_images.reshape(-1, 784) / 255.0\n",
    "        test_images = test_images.reshape(-1, 784) / 255.0\n",
    "        train_labels = np.eye(10)[train_labels]\n",
    "        test_labels = np.eye(10)[test_labels]\n",
    "        \n",
    "        # Split into train/validation\n",
    "        indices = np.arange(train_images.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        train_size = 50000\n",
    "        train_x = train_images[indices[:train_size]]\n",
    "        train_y = train_labels[indices[:train_size]]\n",
    "        val_x = train_images[indices[train_size:]]\n",
    "        val_y = train_labels[indices[train_size:]]\n",
    "        \n",
    "        # Construct layer sizes\n",
    "        layer_size = [784] + [config.hidden_size] * config.num_layers + [10]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = backprop_from_scratch(layer_size, config.activation, config.epsilon)\n",
    "        model.train(train_x, train_y, val_x, val_y, config.epochs, config.learning_rate, config.batch_size,\n",
    "                    config.optimizer, config.momentum, config.beta, config.beta1, config.beta2,\n",
    "                    config.loss, config.weight_decay)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions = model.predict(test_images)\n",
    "        test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))\n",
    "        wandb.log({'test_accuracy': test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461ec381-27fc-4b8a-9d30-90e75786a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10 completed\n",
      "Test Accuracy: 85.05%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# 1. Load and Preprocess the MNIST Fashion Dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_test_onehot = np.eye(num_classes)[y_test]\n",
    "\n",
    "# 2. Initialize Weights and Biases\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(784, 128) * np.sqrt(1 / 784)  # He initialization for ReLU\n",
    "b1 = np.zeros(128)\n",
    "W2 = np.random.randn(128, 10) * np.sqrt(1 / 128)\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "# Initialize velocities for NAG\n",
    "v_W1 = np.zeros_like(W1)\n",
    "v_b1 = np.zeros_like(b1)\n",
    "v_W2 = np.zeros_like(W2)\n",
    "v_b2 = np.zeros_like(b2)\n",
    "\n",
    "# 3. Hyperparameters\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# 4. Activation Functions\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: f(z) = max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation with numerical stability\"\"\"\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# 5. Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train_onehot[indices]\n",
    "    \n",
    "    # Minibatch training\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        actual_batch_size = x_batch.shape[0]  # Handle last batch\n",
    "        \n",
    "        # Forward Pass\n",
    "        z1 = x_batch @ W1 + b1              # (batch_size, 128)\n",
    "        a1 = relu(z1)                       # (batch_size, 128)\n",
    "        z2 = a1 @ W2 + b2                   # (batch_size, 10)\n",
    "        a2 = softmax(z2)                    # (batch_size, 10)\n",
    "        \n",
    "        # Backward Pass (Backpropagation)\n",
    "        # Gradient of loss w.r.t. z2 (cross-entropy with softmax)\n",
    "        dz2 = a2 - y_batch                  # (batch_size, 10)\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        g_W2 = (1 / actual_batch_size) * a1.T @ dz2  # (128, 10)\n",
    "        g_b2 = (1 / actual_batch_size) * np.sum(dz2, axis=0)  # (10,)\n",
    "        \n",
    "        # Backpropagate to hidden layer\n",
    "        da1 = dz2 @ W2.T                    # (batch_size, 128)\n",
    "        dz1 = da1 * (z1 > 0)                # ReLU derivative: 1 if z1 > 0, else 0\n",
    "        g_W1 = (1 / actual_batch_size) * x_batch.T @ dz1  # (784, 128)\n",
    "        g_b1 = (1 / actual_batch_size) * np.sum(dz1, axis=0)  # (128,)\n",
    "        \n",
    "        # Nesterov Accelerated Gradient Descent\n",
    "        # Update velocities\n",
    "        v_W1 = momentum * v_W1 + g_W1\n",
    "        v_b1 = momentum * v_b1 + g_b1\n",
    "        v_W2 = momentum * v_W2 + g_W2\n",
    "        v_b2 = momentum * v_b2 + g_b2\n",
    "        \n",
    "        # Compute NAG updates\n",
    "        d_W1 = g_W1 + momentum * v_W1\n",
    "        d_b1 = g_b1 + momentum * v_b1\n",
    "        d_W2 = g_W2 + momentum * v_W2\n",
    "        d_b2 = g_b2 + momentum * v_b2\n",
    "        \n",
    "        # Update weights and biases\n",
    "        W1 -= learning_rate * d_W1\n",
    "        b1 -= learning_rate * d_b1\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "# 6. Prediction Function\n",
    "def predict(x):\n",
    "    z1 = x @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return np.argmax(a2, axis=1)\n",
    "\n",
    "# 7. Evaluate on Test Set\n",
    "y_pred = predict(x_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73279a7e-69c9-4807-946e-6b9ab2b9e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10 completed\n",
      "Test Accuracy: 78.86%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load and preprocess the MNIST Fashion dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten images (28x28 -> 784) and normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode the labels (10 classes)\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_test_onehot = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Initialize weights and biases with He initialization for ReLU\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(784, 128) * np.sqrt(2 / 784)  # Input to hidden\n",
    "b1 = np.zeros(128)\n",
    "W2 = np.random.randn(128, 10) * np.sqrt(2 / 128)   # Hidden to output\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "# Initialize momentum vectors for NAG\n",
    "v_W1 = np.zeros_like(W1)\n",
    "v_b1 = np.zeros_like(b1)\n",
    "v_W2 = np.zeros_like(W2)\n",
    "v_b2 = np.zeros_like(b2)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define activation functions\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation with numerical stability\"\"\"\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the forward pass\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2, a1, z1  # Return intermediate values for backprop\n",
    "\n",
    "# Training loop with NAG\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train_onehot[indices]\n",
    "    \n",
    "    # Process in minibatches\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        actual_batch_size = x_batch.shape[0]\n",
    "        \n",
    "        # Compute lookahead parameters for NAG\n",
    "        W1_lookahead = W1 -  momentum * v_W1\n",
    "        b1_lookahead = b1 - momentum * v_b1\n",
    "        W2_lookahead = W2 -  momentum * v_W2\n",
    "        b2_lookahead = b2 -  momentum * v_b2\n",
    "        \n",
    "        # Forward pass with lookahead parameters\n",
    "        a2, a1, z1 = forward(x_batch, W1_lookahead, b1_lookahead, W2_lookahead, b2_lookahead)\n",
    "        \n",
    "        # Backward pass: Compute gradients at lookahead point\n",
    "        dz2 = a2 - y_batch  # Gradient of loss w.r.t. z2\n",
    "        g_W2 = a1.T @ dz2 / actual_batch_size\n",
    "        g_b2 = np.sum(dz2, axis=0) / actual_batch_size\n",
    "        \n",
    "        da1 = dz2 @ W2_lookahead.T\n",
    "        dz1 = da1 * (z1 > 0)  # ReLU derivative\n",
    "        g_W1 = x_batch.T @ dz1 / actual_batch_size\n",
    "        g_b1 = np.sum(dz1, axis=0) / actual_batch_size\n",
    "        \n",
    "        # Update momentum with gradients\n",
    "        v_W1 = momentum * v_W1 + g_W1\n",
    "        v_b1 = momentum * v_b1 + g_b1\n",
    "        v_W2 = momentum * v_W2 + g_W2\n",
    "        v_b2 = momentum * v_b2 + g_b2\n",
    "        \n",
    "        # Update parameters\n",
    "        W1 = W1 - learning_rate * v_W1\n",
    "        b1 = b1 - learning_rate * v_b1\n",
    "        W2 = W2 - learning_rate * v_W2\n",
    "        b2 = b2 - learning_rate * v_b2\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "# Define prediction function\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return np.argmax(a2, axis=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = predict(x_test, W1, b1, W2, b2)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0acd28-a11e-4698-8ac5-0f465480aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nag:\n",
    "    def _init_(self):\n",
    "        self.param_update_weights = None\n",
    "        self.param_update_biases = None\n",
    "\n",
    "\n",
    "    def name(self):\n",
    "        return \"nag\"\n",
    "    def optimize_params(self, weights, biases, d_w_array, d_b_array, beta, beta1, beta2, learning_rate, weight_decay, epsilon_for_optimizer, nn, X, y):\n",
    "        if (self.param_update_weights == None):\n",
    "\n",
    "            self.param_update_weights = d_w_array\n",
    "            self.param_update_biases = d_b_array\n",
    "\n",
    "        else:\n",
    "            weight_grad_offsets = [beta * self.param_update_weights[index] for index in range(len(weights))]\n",
    "            bias_grad_offsets = [beta * self.param_update_biases[index] for index in range(len(biases))]\n",
    "            (d_w_array, d_b_array) = nn.backward(X, y, weight_offsets=weight_grad_offsets, bias_offsets=bias_grad_offsets, learning_rate=learning_rate)\n",
    "\n",
    "            self.param_update_weights = [(beta * self.param_update_weights[index]) + d_w_array[index] for index in range(len(d_w_array))]\n",
    "            self.param_update_biases = [(beta * self.param_update_biases[index]) + d_b_array[index] for index in range(len(d_b_array))]\n",
    "\n",
    "        for index in range(len(weights)):\n",
    "            \n",
    "            weights[index] -= (learning_rate * self.param_update_weights[index] - (weight_decay * learning_rate * weights[index]))\n",
    "            biases[index] -= (learning_rate * self.param_update_biases[index] - (weight_decay * learning_rate * biases[index]))\n",
    "        return (weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7292c80-6a89-472d-b9b7-b30e81ca37bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 2.0397\n",
      "Epoch 1/10 completed\n",
      "Epoch 2/10, Average Loss: 1.5841\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10, Average Loss: 1.3568\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10, Average Loss: 1.1346\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10, Average Loss: 0.8181\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10, Average Loss: 0.6513\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10, Average Loss: 0.8904\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10, Average Loss: 0.6604\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10, Average Loss: 0.5728\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10, Average Loss: 0.5373\n",
      "Epoch 10/10 completed\n",
      "Test Accuracy: 78.86%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load and preprocess the MNIST Fashion dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten images (28x28 -> 784) and normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode the labels (10 classes)\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_test_onehot = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Initialize weights and biases with He initialization for ReLU\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(784, 128) * np.sqrt(2 / 784)  # Input to hidden\n",
    "b1 = np.zeros(128)\n",
    "W2 = np.random.randn(128, 10) * np.sqrt(2 / 128)   # Hidden to output\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "# Initialize momentum vectors for NAG\n",
    "v_W1 = np.zeros_like(W1)\n",
    "v_b1 = np.zeros_like(b1)\n",
    "v_W2 = np.zeros_like(W2)\n",
    "v_b2 = np.zeros_like(b2)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define activation functions\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation with numerical stability\"\"\"\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the forward pass\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2, a1, z1  # Return intermediate values for backprop\n",
    "\n",
    "\n",
    "# Training loop with NAG\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train_onehot[indices]\n",
    "    \n",
    "    # Initialize total loss for the epoch\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process in minibatches\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        actual_batch_size = x_batch.shape[0]\n",
    "        \n",
    "        # Compute loss with current parameters\n",
    "        a2_current, _, _ = forward(x_batch, W1, b1, W2, b2)\n",
    "        loss = -np.mean(np.sum(y_batch * np.log(a2_current + 1e-8), axis=1))\n",
    "        total_loss += loss * actual_batch_size\n",
    "        \n",
    "        # Compute lookahead parameters for NAG\n",
    "        W1_lookahead = W1 - momentum * v_W1\n",
    "        b1_lookahead = b1 - momentum * v_b1\n",
    "        W2_lookahead = W2 - momentum * v_W2\n",
    "        b2_lookahead = b2 - momentum * v_b2\n",
    "        \n",
    "        # Forward pass with lookahead parameters\n",
    "        a2, a1, z1 = forward(x_batch, W1_lookahead, b1_lookahead, W2_lookahead, b2_lookahead)\n",
    "        \n",
    "        # Backward pass: Compute gradients at lookahead point\n",
    "        dz2 = a2 - y_batch  # Gradient of loss w.r.t. z2\n",
    "        g_W2 = a1.T @ dz2 / actual_batch_size\n",
    "        g_b2 = np.sum(dz2, axis=0) / actual_batch_size\n",
    "        \n",
    "        da1 = dz2 @ W2_lookahead.T\n",
    "        dz1 = da1 * (z1 > 0)  # ReLU derivative\n",
    "        g_W1 = x_batch.T @ dz1 / actual_batch_size\n",
    "        g_b1 = np.sum(dz1, axis=0) / actual_batch_size\n",
    "        \n",
    "        # Update momentum with gradients\n",
    "        v_W1 = momentum * v_W1 + g_W1\n",
    "        v_b1 = momentum * v_b1 + g_b1\n",
    "        v_W2 = momentum * v_W2 + g_W2\n",
    "        v_b2 = momentum * v_b2 + g_b2\n",
    "        \n",
    "        # Update parameters\n",
    "        W1 = W1 - learning_rate * v_W1\n",
    "        b1 = b1 - learning_rate * v_b1\n",
    "        W2 = W2 - learning_rate * v_W2\n",
    "        b2 = b2 - learning_rate * v_b2\n",
    "    \n",
    "    # Compute and print average loss for the epoch\n",
    "    average_loss = total_loss / len(x_train)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "# Define prediction function\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return np.argmax(a2, axis=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = predict(x_test, W1, b1, W2, b2)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9974568c-f411-4e66-bd18-c87e053a70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 1.9788, Training Accuracy: 31.53%\n",
      "Epoch 2/5, Average Loss: 1.4510, Training Accuracy: 46.48%\n",
      "Epoch 3/5, Average Loss: 1.2815, Training Accuracy: 46.43%\n",
      "Epoch 4/5, Average Loss: 1.2185, Training Accuracy: 47.90%\n",
      "Epoch 5/5, Average Loss: 1.1608, Training Accuracy: 55.76%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Load and preprocess the MNIST Fashion dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten images (28x28 -> 784) and normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# One-hot encode the labels (10 classes)\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_test_onehot = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Initialize weights and biases with He initialization for ReLU\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(784, 128) * np.sqrt(1 / 784)  # Input to hidden\n",
    "b1 = np.zeros(128)\n",
    "W2 = np.random.randn(128, 10) * np.sqrt(1 / 128)   # Hidden to output\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "# Initialize momentum vectors for NAG\n",
    "v_W1 = np.zeros_like(W1)\n",
    "v_b1 = np.zeros_like(b1)\n",
    "v_W2 = np.zeros_like(W2)\n",
    "v_b2 = np.zeros_like(b2)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Define activation functions\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation with numerical stability\"\"\"\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z - z_max)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Define the forward pass\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2, a1, z1  # Return intermediate values for backprop\n",
    "\n",
    "# Training loop with NAG\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train_onehot[indices]\n",
    "    \n",
    "    # Initialize total loss for the epoch\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process data in minibatches\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        actual_batch_size = x_batch.shape[0]\n",
    "        \n",
    "        # Forward pass with current parameters (for loss)\n",
    "        a2_current, _, _ = forward(x_batch, W1, b1, W2, b2)\n",
    "        loss = -np.mean(np.sum(y_batch * np.log(a2_current + 1e-8), axis=1))\n",
    "        total_loss += loss * actual_batch_size\n",
    "        \n",
    "        # Compute lookahead parameters for NAG\n",
    "        W1_lookahead = W1 - momentum * v_W1\n",
    "        b1_lookahead = b1 - momentum * v_b1\n",
    "        W2_lookahead = W2 - momentum * v_W2\n",
    "        b2_lookahead = b2 - momentum * v_b2\n",
    "        \n",
    "        # Forward pass with lookahead parameters\n",
    "        a2, a1, z1 = forward(x_batch, W1_lookahead, b1_lookahead, W2_lookahead, b2_lookahead)\n",
    "        \n",
    "        # Backward pass: Compute gradients at lookahead point\n",
    "        dz2 = a2 - y_batch\n",
    "        g_W2 = a1.T @ dz2 / actual_batch_size\n",
    "        g_b2 = np.sum(dz2, axis=0) / actual_batch_size\n",
    "        \n",
    "        da1 = dz2 @ W2_lookahead.T\n",
    "        dz1 = da1 * (z1 > 0)  # ReLU derivative\n",
    "        g_W1 = x_batch.T @ dz1 / actual_batch_size\n",
    "        g_b1 = np.sum(dz1, axis=0) / actual_batch_size\n",
    "        \n",
    "        # Update momentum\n",
    "        v_W1 = momentum * v_W1 + g_W1\n",
    "        v_b1 = momentum * v_b1 + g_b1\n",
    "        v_W2 = momentum * v_W2 + g_W2\n",
    "        v_b2 = momentum * v_b2 + g_b2\n",
    "        \n",
    "        # Update parameters\n",
    "        W1 = W1 - learning_rate * v_W1\n",
    "        b1 = b1 - learning_rate * v_b1\n",
    "        W2 = W2 - learning_rate * v_W2\n",
    "        b2 = b2 - learning_rate * v_b2\n",
    "    \n",
    "    # Compute average loss for the epoch\n",
    "    average_loss = total_loss / len(x_train)\n",
    "    \n",
    "    # Compute accuracy on the training set\n",
    "    y_pred_train = predict(x_train, W1, b1, W2, b2)\n",
    "    accuracy = np.mean(y_pred_train == y_train)\n",
    "    \n",
    "    # Print loss and accuracy\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}, Training Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19270644-3a3d-4664-8aa8-bc49ec17554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: g9exkpjg\n",
      "Sweep URL: https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/sweeps/g9exkpjg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: h7iuisz9 with config:\n",
      "wandb: \tLearning_rate: 0.001\n",
      "wandb: \tactivation_function: tanh\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tbeta_1: 0.9\n",
      "wandb: \tbeta_2: 0.99\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_num: 3\n",
      "wandb: \thidden_size: 128\n",
      "wandb: \tinitialization: xavier\n",
      "wandb: \tloss_function: mean_squared\n",
      "wandb: \toptimizer: Adam\n",
      "wandb: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Ashutosh Patidar\\OneDrive\\Documents\\GitHub\\Assignment_1_Deep_Learning\\wandb\\run-20250315_194129-h7iuisz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/runs/h7iuisz9' target=\"_blank\">easy-sweep-1</a></strong> to <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/sweeps/g9exkpjg' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/sweeps/g9exkpjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/sweeps/g9exkpjg' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/sweeps/g9exkpjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/runs/h7iuisz9' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/runs/h7iuisz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss:0.2609, val_accuracy: 0.8363\n",
      "epoch: 1, train_loss:0.2050, val_accuracy: 0.8658\n",
      "epoch: 2, train_loss:0.1886, val_accuracy: 0.8696\n",
      "epoch: 3, train_loss:0.1781, val_accuracy: 0.8536\n",
      "epoch: 4, train_loss:0.1709, val_accuracy: 0.8801\n",
      "epoch: 5, train_loss:0.1650, val_accuracy: 0.8688\n",
      "epoch: 6, train_loss:0.1585, val_accuracy: 0.8750\n",
      "epoch: 7, train_loss:0.1523, val_accuracy: 0.8815\n",
      "epoch: 8, train_loss:0.1494, val_accuracy: 0.8803\n",
      "epoch: 9, train_loss:0.1458, val_accuracy: 0.8785\n",
      "test_accuracy: 0.8695\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▆▄█▆▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>test_accuracy</td><td>0.8695</td></tr><tr><td>train_loss</td><td>0.14576</td></tr><tr><td>val_accuracy</td><td>0.8785</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hl_3_bs_32_ac_tanh</strong> at: <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/runs/h7iuisz9' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4/runs/h7iuisz9</a><br> View project at: <a href='https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4' target=\"_blank\">https://wandb.ai/da24s006-indian-institue-of-technology-madras-/Question_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250315_194129-h7iuisz9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.datasets import fashion_mnist # used just to get the data\n",
    "import wandb\n",
    "\n",
    "# defining a backprop_from_scratch to do all.\n",
    "class backprop_from_scratch:\n",
    "\n",
    "    # define constructor\n",
    "    def __init__(self, layer_size, activation_function = 'ReLu', initialization = 'xavier'):\n",
    "        \n",
    "        # initialise the neural network\n",
    "        self.weights, self.biases = [], []\n",
    "        self.num_layers = len(layer_size)\n",
    "        self.layer_sizes = layer_size\n",
    "        self.activation_function = activation_function\n",
    "        self.initialization = initialization\n",
    "        self.initialize_params()\n",
    "        \n",
    "         # declaring and initializing the history for weights and bias\n",
    "        self.history_weights = []\n",
    "        self.history_bias = []\n",
    "\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.history_bias = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # delarin and initializing the velocity for weights and bias\n",
    "        self.weights_velocity = []\n",
    "        self.bias_velocity = []\n",
    "\n",
    "        self.weights_velocity = [np.zeros_like(w) for w in self.weights]\n",
    "        self.bias_velocity = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # setting the beta with there default values\n",
    "        self.beta_1 = 0.900\n",
    "        self.beta_2 = 0.999\n",
    "        \n",
    "    def initialize_params(self):\n",
    "        # let's use He initialization for weights and set values of bias equals to zero\n",
    "        for i in range(self.num_layers-1):\n",
    "            if self.initialization == 'xavier':\n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1/self.layer_sizes[i])\n",
    "            elif self.initialization == 'random': \n",
    "                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) \n",
    "            else: \n",
    "                raise ValueError('invalid initialization method passed')\n",
    "            b = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            self.weights.append(w) \n",
    "            self.biases.append(b) \n",
    "            \n",
    "        # print(self.weights)\n",
    "        # print(self.biases)\n",
    "\n",
    "            \n",
    "    def sigmoid(self, X):\n",
    "        # clipping it bw -500 to 500 \n",
    "        return 1 / (1 + np.exp(-np.clip(X, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x*(1-x) \n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def relu_derivative(self, x): \n",
    "        return np.where(x>0, 1, 0)\n",
    "\n",
    "    def tanh(self, x): \n",
    "       return np.tanh(x) \n",
    "        \n",
    "    def tanh_derivative(self, x): \n",
    "        return 1 - x ** 2\n",
    "    \n",
    "    def softmax(self, X): \n",
    "        # clipping it for numerical stability\n",
    "        exp_x = np.exp(X - np.max(X, axis = 1, keepdims=True))\n",
    "        return exp_x/np.sum(exp_x, axis = 1, keepdims=True)\n",
    "        \n",
    "    def forward_pass(self, data_X, Learning_rate, optimizer='RMS_Prop'): # setting defalut optimizer as RMS_Prop cauz while infering we \n",
    "        # used this function meaning we dont want updated forward pass for NAG, while inferring.\n",
    "        \n",
    "        neuron_outputs = [data_X]\n",
    "        # pass thorugh hidden layers\n",
    "        if optimizer != 'NAG':\n",
    "            for i in range(self.num_layers - 2): \n",
    "        \n",
    "                \n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "                if self.activation_function == 'ReLu':\n",
    "                    h = self.relu(a)\n",
    "                elif self.activation_function == 'Sigmoid':\n",
    "                    h = self.sigmoid(a)\n",
    "                elif self.activation_function == 'tanh':\n",
    "                    h = self.tanh(a)\n",
    "                elif self.activation_function == 'Linear':\n",
    "                    h = a\n",
    "                else: \n",
    "                    raise ValueError('invalide activation passed')\n",
    "                    \n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # pass thorugh the output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1]) + self.biases[-1]\n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "            return neuron_outputs\n",
    "        \n",
    "        else: # if optimizer is NAG\n",
    "            for i in range(self.num_layers - 2): \n",
    "\n",
    "                \n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i] - self.beta_1 * self.weights_velocity[i] * Learning_rate)  # change\n",
    "                + self.biases[i] - self.beta_1 * self.bias_velocity[i] \n",
    "                \n",
    "                if self.activation_function == 'ReLu':\n",
    "                    h = self.relu(a)\n",
    "                elif self.activation_function == 'Sigmoid':\n",
    "                    h = self.sigmoid(a)\n",
    "                elif self.activation_function == 'tanh':\n",
    "                    h = self.tanh(a)\n",
    "                elif self.activation_function == 'Linear':\n",
    "                    h = a\n",
    "                else: \n",
    "                    raise ValueError('invalide activation passed')     \n",
    "                    \n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # pass thorugh the output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1] - self.beta_1 * self.weights_velocity[-1] * Learning_rate) # change\n",
    "            + self.biases[-1] - self.beta_1 * self.bias_velocity[-1]\n",
    "            \n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "            return neuron_outputs\n",
    "\n",
    "    \n",
    "    def backword_pass(self, X, y, neuron_outputs, learning_rate, t, optimizer, loss_function, weight_decay): \n",
    "        \n",
    "        # compute the gradient at given value of params\n",
    "        batch_size = len(X)\n",
    "        \n",
    "        # computing gradies with repect to the output layer\n",
    "        if loss_function == 'categorical_cross_entropy':\n",
    "            delta = neuron_outputs[-1] - y # shape of delta is 32 x 10\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size = len(neuron_outputs[-1])\n",
    "            classes = len(neuron_outputs[-1][0])\n",
    "            # size mismatch problem here\n",
    "            delta = np.zeros((batch_size, classes))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                softmax_jacobian = np.diag(neuron_outputs[-1][i]) - np.outer(neuron_outputs[-1][i], neuron_outputs[-1][i])\n",
    "                # print(softmax_jacobian.shape)\n",
    "                # print(neuron_output[-1][i]\n",
    "                delta[i] = 2 * np.dot(neuron_outputs[-1][i] - y[i], softmax_jacobian)\n",
    "                \n",
    "        if optimizer != 'NAG':\n",
    "            \n",
    "            # computing gradient with respect to hidden layers\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                \n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                \n",
    "                if i>0: # because computing delta for i=0 will be absolutely un-necessary.\n",
    "                    if self.activation_function == 'ReLu': \n",
    "                        delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(neuron_outputs[i])\n",
    "                    elif self.activation_function == 'Sigmoid': \n",
    "                        delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(neuron_outputs[i])\n",
    "                    elif self.activation_function == 'tanh': \n",
    "                        delta = np.dot(delta, self.weights[i].T) * self.tanh_derivative(neuron_outputs[i])\n",
    "                    elif self.activation_function == 'Linear': \n",
    "                        delta = np.dot(delta, self.weights[i].T) * np.ones_like(neuron_outputs[i])\n",
    "    \n",
    "                if optimizer == 'sgd':\n",
    "    \n",
    "                    # making an update\n",
    "                    self.weights[i] -= dw * learning_rate\n",
    "                    self.biases[i] -= db * learning_rate\n",
    "    \n",
    "                    \n",
    "                elif optimizer == 'momentum sgd':\n",
    "                    \n",
    "                    # computing the momentum\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "            \n",
    "                    # making an update\n",
    "                    self.weights[i] -= self.weights_velocity[i] * learning_rate\n",
    "                    self.biases[i] -= self.bias_velocity[i] * learning_rate\n",
    "    \n",
    "                elif optimizer == 'RMS_Prop':\n",
    "                    \n",
    "                    # we need to change the learning rate at each iteration as per history accumulated for the current layer w and b\n",
    "                    self.history_weights[i] = self.beta_1 * self.history_weights[i] + (1 - self.beta_1) * (dw ** 2)\n",
    "                    self.history_bias[i] = self.beta_1 * self.history_bias[i] + (1 - self.beta_1) * (db ** 2) \n",
    "                    self.weights[i] -= dw * learning_rate / np.sqrt(self.history_weights[i] + 1e-12)\n",
    "                    self.biases[i] -= db * learning_rate / np.sqrt(self.history_bias[i] + 1e-12)\n",
    "    \n",
    "                elif optimizer == 'Adam':\n",
    "                    \n",
    "                    # adding the momentum\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db\n",
    "                    \n",
    "                    # we need to change the learning rate at each iteration as per history accumulated for the current layer w and b\n",
    "                    self.history_weights[i] = self.beta_2 * self.history_weights[i] + (1 - self.beta_2) * (dw ** 2)\n",
    "                    self.history_bias[i] = self.beta_2 * self.history_bias[i] + (1 - self.beta_2) * (db ** 2) \n",
    "        \n",
    "                    # implementing the adam rule for update with bias corrected\n",
    "                    self.weights[i] -= self.weights_velocity[i]/(1 - self.beta_1 ** t) * learning_rate / np.sqrt(self.history_weights[i] / (1 - self.beta_2** t) + 1e-12)\n",
    "                    self.biases[i] -= self.bias_velocity[i]/(1 - self.beta_1 ** t) * learning_rate / np.sqrt(self.history_bias[i] / (1 - self.beta_2 ** t) + 1e-12) \n",
    "    \n",
    "                elif optimizer == 'Nadam':\n",
    "                    \n",
    "                    # adding the momentum\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db\n",
    "        \n",
    "                    # we need to change the learning rate at each iteration as per history accumulated for the current layer w&b\n",
    "                    self.history_weights[i] = self.beta_2 * self.history_weights[i] + (1 - self.beta_2) * (dw ** 2)\n",
    "                    self.history_bias[i] = self.beta_2 * self.history_bias[i] + (1 - self.beta_2) * (db ** 2) \n",
    "                    \n",
    "                    # implementing the nadam rule for update with bias corrected\n",
    "                    self.weights[i] -= ((self.beta_1 * self.weights_velocity[i]) + (1 - self.beta_1) * dw)/(1 - self.beta_1 ** t) * learning_rate / np.sqrt(self.history_weights[i] / (1 - self.beta_2** t) + 1e-12)\n",
    "                    self.biases[i] -= ((self.beta_1 * self.bias_velocity[i]) + (1 - self.beta_1) * db)/(1 - self.beta_1 ** t) * learning_rate / np.sqrt(self.history_bias[i] / (1 - self.beta_2 ** t) + 1e-12) \n",
    "                    \n",
    "        else: # if optimizer is 'NAG'\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                \n",
    "                # neuron_ouputs depend on the data, which we have computed in the forward pass\n",
    "                # but delta will depend on on the weight matrices\n",
    "                \n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                \n",
    "                if i>0: # because computing delta for i=0 will be absolutely un-necessary.\n",
    "                    if self.activation_function == 'ReLu': # changed 4 places\n",
    "                        delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i] * learning_rate).T) * self.relu_derivative(neuron_outputs[i])\n",
    "                    elif self.activation_function == 'Sigmoid':\n",
    "                        delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i] * learning_rate).T) * self.sigmoid_derivative(neuron_outputs[i])\n",
    "                    elif self.activation_function == 'tanh':\n",
    "                        delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i] * learning_rate).T) * self.tanh_derivative(neuron_outputs[i])    \n",
    "                    elif self.activation_function == 'Linear':\n",
    "                        delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i] * learning_rate).T) * np.ones_like(neuron_outputs[i])\n",
    "                        \n",
    "                # computing the momentum\n",
    "                self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "    \n",
    "                # making an update\n",
    "                self.weights[i] -= self.weights_velocity[i] * learning_rate\n",
    "                self.biases[i] -= self.bias_velocity[i] * learning_rate\n",
    "    \n",
    "    # function to train the network\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, learning_rate=0.001, batch_size=32, optimizer=\"NAG\", beta_1=0.900, beta_2=0.999, loss_function='categorical_cross_entropy', weight_decay = 0, activation_function='ReLu'): # setting RMS_Prop as default optimizer\n",
    "        \n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # first shuffle the training data\n",
    "            # print(X_train.shape[0])\n",
    "            \n",
    "            indices = np.random.permutation(X_train.shape[0]) \n",
    "            X_train_permuted = X_train[indices]\n",
    "            y_train_permuted = y_train[indices]\n",
    "            \n",
    "            total_loss = 0\n",
    "            batch_num = 0\n",
    "            \n",
    "            # let's make a update for a mini batch\n",
    "            for i in range(0, X_train.shape[0], batch_size): \n",
    "                batch_x = X_train_permuted[i: i + batch_size]\n",
    "                batch_y = y_train_permuted[i: i + batch_size]\n",
    "\n",
    "                # calling forward and remember it's different for NAG so passing optmizer in it, to check inside it it's NAG\n",
    "                neuron_outputs = self.forward_pass(batch_x, learning_rate, optimizer)\n",
    "                \n",
    "                l2_norm_weights, l2_norm_bias = 0, 0\n",
    "                for i in range(len(self.weights)):\n",
    "                    l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "                for i in range(len(self.biases)):\n",
    "                    l2_norm_bias += np.sum(self.biases[i] ** 2) \n",
    "                l2_norm_params = l2_norm_bias + l2_norm_weights\n",
    "                \n",
    "                if loss_function == 'categorical_cross_entropy':\n",
    "                    loss = -np.mean(np.sum(batch_y * np.log(neuron_outputs[-1] + 1e-10), axis = 1)) +  (weight_decay/2) * l2_norm_params \n",
    "                    # added 1e-10 to prevent num underflow\n",
    "                    # this loss in not interpretable in case of NAG as this will compute the forward pass for update weights\n",
    "                else:\n",
    "                    loss = np.mean(np.sum((batch_y - neuron_outputs[-1]) ** 2, axis = 1)) + (weight_decay/2) * l2_norm_params\n",
    "                    # loss function for MSE\n",
    "            \n",
    "                total_loss += loss\n",
    "                batch_num += 1\n",
    "                \n",
    "                # backprop: computing the gradient and making an update in backwork pass func\n",
    "                self.backword_pass(batch_x, batch_y, neuron_outputs, learning_rate, batch_num, optimizer, loss_function, weight_decay)\n",
    "            \n",
    "            average_loss = total_loss/batch_num\n",
    "            \n",
    "            # now let's make predictions on validation dataset \n",
    "            validation_predictions = self.predict(X_val, learning_rate)\n",
    "            validation_accuracy = np.mean(validation_predictions == np.argmax(y_val, axis = 1))\n",
    "\n",
    "            # loging to wandb\n",
    "            wandb.log({'epoch': epoch , 'train_loss': average_loss, 'val_accuracy': validation_accuracy})\n",
    "            print(f\"epoch: {epoch}, train_loss:{average_loss:.4f}, val_accuracy: {validation_accuracy:.4f}\")\n",
    "            \n",
    "    def predict(self, X,learning_rate): \n",
    "        # this will predict class labels for the passed data\n",
    "        neuron_outputs = self.forward_pass(X, learning_rate)\n",
    "        return np.argmax(neuron_outputs[-1], axis=1)\n",
    "\n",
    "def sweep_train():\n",
    "\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    sweep_name = f\"hl_{config.hidden_num}_bs_{config.batch_size}_ac_{config.activation_function}\"\n",
    "    wandb.run.name = sweep_name\n",
    "    \n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    # to reproduce the results\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    # making the data ready to train the model\n",
    "    \n",
    "    # Splitting the trainig data into train and validation\n",
    "    indices = np.arange(train_images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    train_size = 50000\n",
    "    \n",
    "    train_x = train_images[indices[:train_size]]\n",
    "    train_y = train_labels[indices[:train_size]]\n",
    "    val_x = train_images[indices[train_size:]]\n",
    "    val_y = train_labels[indices[train_size:]]\n",
    "\n",
    "    train_x = train_x.reshape(train_x.shape[0], -1) / 255\n",
    "    val_x = val_x.reshape(val_x.shape[0], -1) / 255\n",
    "\n",
    "    # converting y's into one hot vector\n",
    "    num_classes = 10\n",
    "    train_y = np.eye(num_classes)[train_y]\n",
    "    val_y = np.eye(num_classes)[val_y]\n",
    "\n",
    "    # let's do it for test data as well\n",
    "    test_images = test_images.reshape(test_images.shape[0], -1) / 255\n",
    "    test_labels = np.eye(num_classes)[test_labels]\n",
    "\n",
    "    # now let's create and train the network\n",
    "    model = backprop_from_scratch([784] + [config.hidden_size] * config.hidden_num + [10], \n",
    "                                  config.activation_function, \n",
    "                                  config.initialization)\n",
    "    \n",
    "    model.train(train_x, \n",
    "                train_y, \n",
    "                val_x, \n",
    "                val_y, \n",
    "                config.epochs, \n",
    "                config.Learning_rate, \n",
    "                config.batch_size, \n",
    "                config.optimizer, \n",
    "                config.beta_1, \n",
    "                config.beta_2, \n",
    "                config.loss_function, \n",
    "                config.weight_decay, \n",
    "                config.activation_function\n",
    "               )\n",
    "\n",
    "    # now let's evaluate on the test set\n",
    "    test_predictions = model.predict(test_images, config.Learning_rate) \n",
    "    test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis = 1))\n",
    "    print(f\"test_accuracy:{test_accuracy: .4f}\") \n",
    "    wandb.log({'test_accuracy': test_accuracy})\n",
    "\n",
    "# sweep configs\n",
    "sweep_configurations = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "        \"name\": \"val_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"values\": [5, 10]},\n",
    "        \"hidden_num\": {\"values\": [3, 4, 5]},\n",
    "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 0.0005, 0.5]},\n",
    "        \"Learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
    "        \"optimizer\": {\"values\": [\"sgd\", \"momentum sgd\", \"NAG\", \"RMS_Prop\", \"Adam\", \"Nadam\"]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"initialization\": {\"values\": [\"random\", \"xavier\"]},\n",
    "        \"activation_function\": {\"values\": [\"Sigmoid\", \"tanh\", \"ReLu\"]},\n",
    "        \"beta_1\": {\"values\": [0.90]},\n",
    "        \"beta_2\": {\"values\": [0.99]},\n",
    "        \"loss_function\": {\"values\": ['categorical_cross_entropy', 'mean_squared']}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # initializing hyperparms in wandb\n",
    "\n",
    "    # epochs = [5, 10]\n",
    "    # hidden_num = [3, 4, 5]\n",
    "    # hidden_size = [32, 64, 128]\n",
    "    # weight_decay = [0, 0.0005, 0.5]\n",
    "    # Learning_rate = [1e-3, 1e-4]\n",
    "    # optimizers = ['sgd', 'momentum sgd','NAG','RMS_Prop','Adam','Nadam']\n",
    "    # batch_size = [16, 32, 64]\n",
    "    # initialization = ['xavier', 'random'] # introduced xavier and random initialization\n",
    "    # activation_function = ['ReLu', 'Sigmoid', 'Linear', 'tanh']\n",
    "    # loss = ['mean_squared', 'categorical_cross_entropy']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # wandb.init(project='backprop_scratch', \n",
    "    #         config={ 'Learning_rate' : 0.001, \n",
    "    #                     'epochs' : 50, \n",
    "    #                     'batch_size' : 32, \n",
    "    #                     'layer_size' : [784, 64, 64, 10], \n",
    "    #                 'optimizer': 'RMS_Prop',\n",
    "    #                 'beta_1': 0.900,         # beta 1 is for momentum sgd, NAG, RMS_Prop\n",
    "    #                 'beta_2': 0.999,         # use beta 2 for Nadam and Adam\n",
    "    #                'loss_function': 'mean_squared',\n",
    "    #                 'weight_decay': 0,\n",
    "    #                 'activation_function': 'ReLu',\n",
    "    #                 'initialization': 'random'\n",
    "    #                }         \n",
    "    #         )\n",
    "\n",
    "    # config.hidden_size = 64\n",
    "    # config.hidden_num = 2\n",
    "    # config.layer_size = [784] + [config.hidden_size] * config.hidden_num + [10]\n",
    "    # config.beta_1 = 0.900\n",
    "    # config.beta_2 = 0.999\n",
    "    # config.epochs = 2\n",
    "    # config.loss_function = 'categorical_cross_entropy'\n",
    "    # config.weight_decay = 0\n",
    "    # config.optimizer = 'NAG'\n",
    "    # config.Learning_rate = 0.01\n",
    "    # config.activation_function = 'ReLu'\n",
    "    # config.initialization = 'xavier'\n",
    "    # config.batch_size = 32\n",
    "    \n",
    "    # config = wandb.config\n",
    "    # print(wandb.config)\n",
    "\n",
    "\n",
    "# executing sweep\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating a sweep\n",
    "    sweep_id = wandb.sweep(sweep_configurations, project=\"Question_4\")\n",
    "    # starting sweep agent\n",
    "    wandb.agent(sweep_id, function=sweep_train)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32fd6-52f8-4472-9c78-fe31621460d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
