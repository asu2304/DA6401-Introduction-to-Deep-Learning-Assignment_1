{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e46e56-b9f2-4ac2-992f-0975c6cbaed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learning_rate': 0.001, 'epochs': 10, 'batch_size': 32, 'layer_size': [784, 128, 64, 10], 'optimizer': 'Rms_prop', 'beta_1': 0.9, 'beta_2': 0.999, 'loss_function': 'categorical_cross_entropy', 'weight_decay': 0, 'activation_function': 'Sigmoid', 'initialization': 'He'}\n",
      "epoch: 0, train_loss:0.6159, val_accuracy: 0.8391\n",
      "epoch: 1, train_loss:0.4048, val_accuracy: 0.8613\n",
      "epoch: 2, train_loss:0.3670, val_accuracy: 0.8539\n",
      "epoch: 3, train_loss:0.3443, val_accuracy: 0.8615\n",
      "epoch: 4, train_loss:0.3278, val_accuracy: 0.8762\n",
      "epoch: 5, train_loss:0.3140, val_accuracy: 0.8784\n",
      "epoch: 6, train_loss:0.3021, val_accuracy: 0.8562\n",
      "epoch: 7, train_loss:0.2913, val_accuracy: 0.8806\n",
      "epoch: 8, train_loss:0.2816, val_accuracy: 0.8830\n",
      "epoch: 9, train_loss:0.2732, val_accuracy: 0.8895\n",
      "test_accuracy: 0.8809\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "\n",
    "class backprop_from_scratch:\n",
    "    \n",
    "    def __init__(self, layer_size, activation_function='sigmoid'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the neural network with the specified layer sizes and activation function.\n",
    "        \n",
    "        Args:\n",
    "            layer_size (list): List of integers specifying the size of each layer.\n",
    "            activation_function (str): Activation function for hidden layers ('sigmoid', 'relu', 'tanh', 'linear').\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate activation function\n",
    "        valid_activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
    "        if activation_function.lower() not in valid_activations:\n",
    "            raise ValueError(f\"Activation function must be one of {valid_activations}, got {activation_function}\")\n",
    "        self.activation_function = activation_function.lower()\n",
    "        \n",
    "        # Initialize network parameters\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.num_layers = len(layer_size)\n",
    "        self.layer_sizes = layer_size\n",
    "        self.initialize_params()\n",
    "        \n",
    "        # Initialize histories and velocities for optimizers\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.history_bias = [np.zeros_like(b) for b in self.biases]\n",
    "        self.weights_velocity = [np.zeros_like(w) for w in self.weights]\n",
    "        self.bias_velocity = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Default beta values for optimizers\n",
    "        self.beta_1 = 0.900\n",
    "        self.beta_2 = 0.999\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"Initialize weights with He initialization and biases with zeros.\"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            b = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def apply_activation(self, a):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function to the input.\n",
    "        \n",
    "        Args:\n",
    "            a (np.ndarray): Input array to apply the activation function to.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Activated output.\n",
    "        \"\"\"\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(a)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return self.relu(a)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh(a)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear(a)\n",
    "\n",
    "    def get_activation_derivative(self, h):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function based on the output.\n",
    "        \n",
    "        Args:\n",
    "            h (np.ndarray): Output of the activation function.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return h * (1 - h)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return (h > 0).astype(float)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return 1 - h ** 2\n",
    "        elif self.activation_function == 'linear':\n",
    "            return np.ones_like(h)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function, vectorized.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function, vectorized.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function, vectorized.\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear activation function (identity), vectorized.\"\"\"\n",
    "        return x\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for the output layer, vectorized.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward_pass(self, data_x, optimizer='rms_prop'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            data_x (np.ndarray): Input data.\n",
    "            optimizer (str): Optimizer type to determine weight updates.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of neuron outputs at each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        neuron_outputs = [data_x]\n",
    "        if optimizer.lower() != 'nag':\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "                h = self.apply_activation(a)\n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # Output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1]) + self.biases[-1]\n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "            \n",
    "        else:  # NAG optimizer\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = (np.dot(neuron_outputs[-1], self.weights[i] - self.beta_1 * self.weights_velocity[i]) +\n",
    "                     self.biases[i] - self.beta_1 * self.bias_velocity[i])\n",
    "                h = self.apply_activation(a)\n",
    "                neuron_outputs.append(h)\n",
    "                \n",
    "            # Output layer\n",
    "            a = (np.dot(neuron_outputs[-1], self.weights[-1] - self.beta_1 * self.weights_velocity[-1]) +\n",
    "                 self.biases[-1] - self.beta_1 * self.bias_velocity[-1])\n",
    "            output = self.softmax(a)\n",
    "            neuron_outputs.append(output)\n",
    "        return neuron_outputs\n",
    "\n",
    "    def backward_pass(self, x, y, neuron_outputs, learning_rate, t, optimizer, loss_function, weight_decay):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to update weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            x (np.ndarray): Input batch.\n",
    "            y (np.ndarray): True labels.\n",
    "            neuron_outputs (list): Outputs from the forward pass.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            t (int): Current iteration for bias correction in optimizers.\n",
    "            optimizer (str): Optimizer type.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): Weight decay parameter.\n",
    "        \"\"\"\n",
    "        batch_size = len(x)\n",
    "        # Compute initial delta for output layer\n",
    "        if loss_function.lower() == 'categorical_cross_entropy':\n",
    "            delta = neuron_outputs[-1] - y\n",
    "        else:  # Mean squared error\n",
    "            delta = np.zeros((batch_size, self.layer_sizes[-1]))\n",
    "            for i in range(batch_size):\n",
    "                softmax_jacobian = (np.diag(neuron_outputs[-1][i]) -\n",
    "                                   np.outer(neuron_outputs[-1][i], neuron_outputs[-1][i]))\n",
    "                delta[i] = 2 * np.dot(neuron_outputs[-1][i] - y[i], softmax_jacobian)\n",
    "\n",
    "        # Update weights and biases layer by layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                \n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                \n",
    "                if i > 0:\n",
    "                    delta = np.dot(delta, self.weights[i].T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "\n",
    "                if optimizer.lower() == 'sgd':\n",
    "                    self.weights[i] -= learning_rate * dw\n",
    "                    self.biases[i] -= learning_rate * db\n",
    "                    \n",
    "                elif optimizer.lower() == 'momentum sgd':\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                    self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                    self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "                    \n",
    "                elif optimizer.lower() == 'rms_prop':\n",
    "                    self.history_weights[i] = (self.beta_1 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_1) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_1 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_1) * (db ** 2))\n",
    "                    self.weights[i] -= learning_rate * dw / np.sqrt(self.history_weights[i] + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * db / np.sqrt(self.history_bias[i] + 1e-12)\n",
    "                    \n",
    "                elif optimizer.lower() == 'adam':\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    w_vel_corr = self.weights_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    b_vel_corr = self.bias_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    self.weights[i] -= learning_rate * w_vel_corr / np.sqrt(w_hist_corr + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * b_vel_corr / np.sqrt(b_hist_corr + 1e-12)\n",
    "                    \n",
    "                elif optimizer.lower() == 'nadam':\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    w_momentum = (self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw) / (1 - self.beta_1 ** t)\n",
    "                    b_momentum = (self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db) / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    self.weights[i] -= learning_rate * w_momentum / np.sqrt(w_hist_corr + 1e-12)\n",
    "                    self.biases[i] -= learning_rate * b_momentum / np.sqrt(b_hist_corr + 1e-12)\n",
    "                    \n",
    "        else:  # NAG optimizer\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:\n",
    "                    delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i]).T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "                self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=50, learning_rate=0.001, batch_size=32,\n",
    "              optimizer=\"nag\", beta_1=0.900, beta_2=0.999, loss_function='categorical_cross_entropy',\n",
    "              weight_decay=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "            x_train (np.ndarray): Training input data.\n",
    "            y_train (np.ndarray): Training labels.\n",
    "            x_val (np.ndarray): Validation input data.\n",
    "            y_val (np.ndarray): Validation labels.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate.\n",
    "            batch_size (int): Size of each mini-batch.\n",
    "            optimizer (str): Optimizer type.\n",
    "            beta_1 (float): Momentum parameter.\n",
    "            beta_2 (float): RMSProp/Adam parameter.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): Weight decay parameter.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(x_train.shape[0])\n",
    "            \n",
    "            x_train_permuted = x_train[indices]\n",
    "            y_train_permuted = y_train[indices]\n",
    "            total_loss = 0\n",
    "            batch_num = 0\n",
    "            \n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                \n",
    "                batch_x = x_train_permuted[i:i + batch_size]\n",
    "                batch_y = y_train_permuted[i:i + batch_size]\n",
    "                neuron_outputs = self.forward_pass(batch_x, optimizer)\n",
    "                l2_norm_params = sum(np.sum(w ** 2) for w in self.weights) + sum(np.sum(b ** 2) for b in self.biases)\n",
    "                if loss_function.lower() == 'categorical_cross_entropy':\n",
    "                    loss = (-np.mean(np.sum(batch_y * np.log(neuron_outputs[-1] + 1e-10), axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                else:\n",
    "                    loss = (np.mean(np.sum((batch_y - neuron_outputs[-1]) ** 2, axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                total_loss += loss\n",
    "                batch_num += 1\n",
    "                self.backward_pass(batch_x, batch_y, neuron_outputs, learning_rate, batch_num, optimizer,\n",
    "                                  loss_function, weight_decay)\n",
    "                \n",
    "            average_loss = total_loss / batch_num\n",
    "            validation_predictions = self.predict(x_val)\n",
    "            validation_accuracy = np.mean(validation_predictions == np.argmax(y_val, axis=1))\n",
    "            wandb.log({'epoch': epoch, 'train_loss': average_loss, 'val_accuracy': validation_accuracy})\n",
    "            print(f\"epoch: {epoch}, train_loss:{average_loss:.4f}, val_accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for the input data.\n",
    "        \n",
    "        Args:\n",
    "            x (np.ndarray): Input data.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Predicted class indices.\n",
    "        \"\"\"\n",
    "        neuron_outputs = self.forward_pass(x)\n",
    "        return np.argmax(neuron_outputs[-1], axis=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    wandb.login()\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    # Prepare data\n",
    "    indices = np.arange(train_images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    train_size = 50000\n",
    "    \n",
    "    train_x = train_images[indices[:train_size]].reshape(-1, 784) / 255\n",
    "    train_y = np.eye(10)[train_labels[indices[:train_size]]]\n",
    "    val_x = train_images[indices[train_size:]].reshape(-1, 784) / 255\n",
    "    val_y = np.eye(10)[train_labels[indices[train_size:]]]\n",
    "    \n",
    "    test_images = test_images.reshape(-1, 784) / 255\n",
    "    test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(project='backprop_scratch', config={\n",
    "        'Learning_rate': 0.001,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'layer_size': [784, 128, 64, 10],\n",
    "        'optimizer': 'rms_prop',\n",
    "        'beta_1': 0.900,\n",
    "        'beta_2': 0.999,\n",
    "        'loss_function': 'categorical_cross_entropy',\n",
    "        'weight_decay': 0,\n",
    "        'activation_function': 'relu'\n",
    "    })\n",
    "    \n",
    "    config = wandb.config\n",
    "    print(wandb.config)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = backprop_from_scratch(config.layer_size, config.activation_function)\n",
    "    model.train(train_x, train_y, val_x, val_y, config.epochs, config.Learning_rate, config.batch_size,\n",
    "                config.optimizer, config.beta_1, config.beta_2, config.loss_function, config.weight_decay)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_predictions = model.predict(test_images)\n",
    "    test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))\n",
    "    print(f\"test_accuracy: {test_accuracy:.4f}\")\n",
    "    wandb.log({'test_accuracy': test_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175f90a7-f708-4bb8-85cf-efd3a01165a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset {mnist,fashion_mnist}] [--hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]]\n",
      "                             [--activation_function {sigmoid,relu,tanh,linear}] [--learning_rate LEARNING_RATE]\n",
      "                             [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--optimizer {sgd,momentum sgd,nag,rms_prop,adam,nadam}] [--beta_1 BETA_1]\n",
      "                             [--beta_2 BETA_2] [--loss_function {categorical_cross_entropy,mean_squared_error}]\n",
      "                             [--weight_decay WEIGHT_DECAY] [--epsilon EPSILON]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Ashutosh Patidar\\AppData\\Roaming\\jupyter\\runtime\\kernel-81958830-bda1-409d-85de-4425b3321a1b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.datasets import mnist  # Added to support MNIST dataset\n",
    "from keras.datasets import fashion_mnist  # Original dataset import\n",
    "\n",
    "import wandb  # For logging and sweep functionality\n",
    "import argparse  # For parsing command-line arguments\n",
    "\n",
    "# Define the neural network class\n",
    "class backprop_from_scratch:\n",
    "    \n",
    "    def __init__(self, layer_size, activation_function='sigmoid', epsilon=1e-8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the neural network with layer sizes, activation function, and epsilon.\n",
    "\n",
    "        Args:\n",
    "            layer_size (list): List of integers specifying the size of each layer (input, hidden, output).\n",
    "            activation_function (str): Activation function for hidden layers ('sigmoid', 'relu', 'tanh', 'linear').\n",
    "            epsilon (float): Small value for numerical stability in computations (e.g., avoiding log(0)).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate activation function to ensure it’s supported\n",
    "        valid_activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
    "        if activation_function.lower() not in valid_activations:\n",
    "            raise ValueError(f\"Activation function must be one of {valid_activations}, got {activation_function}\")\n",
    "        self.activation_function = activation_function.lower()  # Store as lowercase for consistency\n",
    "        self.epsilon = epsilon  # Store epsilon for use in loss and optimizer calculations\n",
    "        \n",
    "        # Initialize network architecture and parameters\n",
    "        self.weights = []  # List to hold weight matrices\n",
    "        self.biases = []  # List to hold bias vectors\n",
    "        self.num_layers = len(layer_size)  # Total number of layers (input + hidden + output)\n",
    "        self.layer_sizes = layer_size  # Store layer sizes for reference\n",
    "        self.initialize_params()  # Initialize weights and biases\n",
    "        \n",
    "        # Initialize optimizer histories and velocities with zeros, matching weight/bias shapes\n",
    "        self.history_weights = [np.zeros_like(w) for w in self.weights]  # For RMSProp/Adam history\n",
    "        self.history_bias = [np.zeros_like(b) for b in self.biases]  # For RMSProp/Adam history\n",
    "        self.weights_velocity = [np.zeros_like(w) for w in self.weights]  # For momentum/NAG\n",
    "        self.bias_velocity = [np.zeros_like(b) for b in self.biases]  # For momentum/NAG\n",
    "        \n",
    "        # Default beta values for optimizers (will be overridden if provided)\n",
    "        self.beta_1 = 0.900  # Momentum/RMSProp/Adam beta1\n",
    "        self.beta_2 = 0.999  # Adam/Nadam beta2\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"Initialize weights with He initialization and biases with zeros.\"\"\"\n",
    "        # Loop through layers to create weight matrices and bias vectors\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # He initialization for weights: scales by sqrt(2/input_size) for ReLU compatibility\n",
    "            w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            b = np.zeros((1, self.layer_sizes[i + 1]))  # Biases initialized to zero\n",
    "            self.weights.append(w)  # Add weight matrix to list\n",
    "            self.biases.append(b)  # Add bias vector to list\n",
    "\n",
    "    def apply_activation(self, a):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function to the input.\n",
    "\n",
    "        Args:\n",
    "            a (np.ndarray): Input array (pre-activation values).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Activated output.\n",
    "        \"\"\"\n",
    "        # Select activation function based on self.activation_function\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(a)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return self.relu(a)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh(a)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear(a)\n",
    "\n",
    "    def get_activation_derivative(self, h):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function based on its output.\n",
    "\n",
    "        Args:\n",
    "            h (np.ndarray): Output of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        # Compute derivative based on activation function type\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return h * (1 - h)  # Sigmoid derivative: h * (1 - h)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return (h > 0).astype(float)  # ReLU derivative: 1 if h > 0, else 0\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return 1 - h ** 2  # Tanh derivative: 1 - h^2\n",
    "        elif self.activation_function == 'linear':\n",
    "            return np.ones_like(h)  # Linear derivative: constant 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function with clipping to prevent overflow.\"\"\"\n",
    "        # Clip input to [-500, 500] to avoid exp overflow, then compute sigmoid\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)  # Return max(0, x) element-wise\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(x)  # NumPy’s built-in tanh function\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear (identity) activation function.\"\"\"\n",
    "        return x  # Simply return the input unchanged\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for the output layer.\"\"\"\n",
    "        # Subtract max per row for numerical stability, then compute softmax\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "\n",
    "    def forward_pass(self, data_x, optimizer='rms_prop'):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            data_x (np.ndarray): Input data (batch_size x input_size).\n",
    "            optimizer (str): Optimizer type, affects NAG computation.\n",
    "\n",
    "        Returns:\n",
    "            list: Neuron outputs at each layer (including input and output).\n",
    "        \"\"\"\n",
    "        neuron_outputs = [data_x]  # Start with input layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            # Standard forward pass for non-NAG optimizers\n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                a = np.dot(neuron_outputs[-1], self.weights[i]) + self.biases[i]  # Linear transformation\n",
    "                h = self.apply_activation(a)  # Apply activation\n",
    "                neuron_outputs.append(h)  # Store output\n",
    "            # Output layer\n",
    "            a = np.dot(neuron_outputs[-1], self.weights[-1]) + self.biases[-1]  # Linear transformation\n",
    "            output = self.softmax(a)  # Apply softmax for probabilities\n",
    "            neuron_outputs.append(output)  # Store output\n",
    "        else:\n",
    "            # NAG forward pass: look-ahead using velocity\n",
    "            # Hidden layers\n",
    "            for i in range(self.num_layers - 2):\n",
    "                # Adjust weights and biases with momentum terms\n",
    "                a = (np.dot(neuron_outputs[-1], self.weights[i] - self.beta_1 * self.weights_velocity[i]) +\n",
    "                     self.biases[i] - self.beta_1 * self.bias_velocity[i])\n",
    "                h = self.apply_activation(a)  # Apply activation\n",
    "                neuron_outputs.append(h)  # Store output\n",
    "            # Output layer\n",
    "            a = (np.dot(neuron_outputs[-1], self.weights[-1] - self.beta_1 * self.weights_velocity[-1]) +\n",
    "                 self.biases[-1] - self.beta_1 * self.bias_velocity[-1])\n",
    "            output = self.softmax(a)  # Apply softmax\n",
    "            neuron_outputs.append(output)  # Store output\n",
    "        return neuron_outputs\n",
    "\n",
    "    def backward_pass(self, x, y, neuron_outputs, learning_rate, t, optimizer, loss_function, weight_decay):\n",
    "        \"\"\"\n",
    "        Perform the backward pass to update weights and biases.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input batch.\n",
    "            y (np.ndarray): True labels (one-hot encoded).\n",
    "            neuron_outputs (list): Outputs from forward pass.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            t (int): Current iteration (for bias correction in Adam/Nadam).\n",
    "            optimizer (str): Optimizer type (sgd, momentum sgd, nag, rms_prop, adam, nadam).\n",
    "            loss_function (str): Loss function (categorical_cross_entropy or mean_squared_error).\n",
    "            weight_decay (float): L2 regularization parameter.\n",
    "        \"\"\"\n",
    "        batch_size = len(x)  # Number of samples in the batch\n",
    "        # Compute initial delta for output layer\n",
    "        if loss_function.lower() == 'categorical_cross_entropy':\n",
    "            delta = neuron_outputs[-1] - y  # Gradient of cross-entropy w.r.t. softmax output\n",
    "        else:  # Mean squared error\n",
    "            delta = np.zeros((batch_size, self.layer_sizes[-1]))  # Initialize delta array\n",
    "            for i in range(batch_size):\n",
    "                # Compute softmax Jacobian for MSE gradient\n",
    "                softmax_jacobian = (np.diag(neuron_outputs[-1][i]) -\n",
    "                                   np.outer(neuron_outputs[-1][i], neuron_outputs[-1][i]))\n",
    "                delta[i] = 2 * np.dot(neuron_outputs[-1][i] - y[i], softmax_jacobian)  # MSE gradient\n",
    "\n",
    "        # Update weights and biases from output to input layer\n",
    "        if optimizer.lower() != 'nag':\n",
    "            for i in range(self.num_layers - 2, -1, -1):  # Iterate backwards\n",
    "                # Compute gradients with weight decay\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:  # If not the input layer\n",
    "                    # Propagate delta backwards\n",
    "                    delta = np.dot(delta, self.weights[i].T) * self.get_activation_derivative(neuron_outputs[i])\n",
    "\n",
    "                # Update parameters based on optimizer\n",
    "                if optimizer.lower() == 'sgd':\n",
    "                    self.weights[i] -= learning_rate * dw  # Simple gradient descent\n",
    "                    self.biases[i] -= learning_rate * db\n",
    "                elif optimizer.lower() == 'momentum sgd':\n",
    "                    # Momentum updates\n",
    "                    self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                    self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                    self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                    self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "                elif optimizer.lower() == 'rms_prop':\n",
    "                    # RMSProp: update history with moving average of squared gradients\n",
    "                    self.history_weights[i] = (self.beta_1 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_1) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_1 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_1) * (db ** 2))\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * dw / np.sqrt(self.history_weights[i] + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * db / np.sqrt(self.history_bias[i] + self.epsilon)\n",
    "                elif optimizer.lower() == 'adam':\n",
    "                    # Adam: update velocity (first moment) and history (second moment)\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    # Bias-corrected estimates\n",
    "                    w_vel_corr = self.weights_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    b_vel_corr = self.bias_velocity[i] / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * w_vel_corr / np.sqrt(w_hist_corr + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * b_vel_corr / np.sqrt(b_hist_corr + self.epsilon)\n",
    "                elif optimizer.lower() == 'nadam':\n",
    "                    # Nadam: Nesterov + Adam\n",
    "                    self.weights_velocity[i] = (self.beta_1 * self.weights_velocity[i] +\n",
    "                                               (1 - self.beta_1) * dw)\n",
    "                    self.bias_velocity[i] = (self.beta_1 * self.bias_velocity[i] +\n",
    "                                            (1 - self.beta_1) * db)\n",
    "                    self.history_weights[i] = (self.beta_2 * self.history_weights[i] +\n",
    "                                              (1 - self.beta_2) * (dw ** 2))\n",
    "                    self.history_bias[i] = (self.beta_2 * self.history_bias[i] +\n",
    "                                           (1 - self.beta_2) * (db ** 2))\n",
    "                    # Nesterov momentum with bias correction\n",
    "                    w_momentum = (self.beta_1 * self.weights_velocity[i] + (1 - self.beta_1) * dw) / (1 - self.beta_1 ** t)\n",
    "                    b_momentum = (self.beta_1 * self.bias_velocity[i] + (1 - self.beta_1) * db) / (1 - self.beta_1 ** t)\n",
    "                    w_hist_corr = self.history_weights[i] / (1 - self.beta_2 ** t)\n",
    "                    b_hist_corr = self.history_bias[i] / (1 - self.beta_2 ** t)\n",
    "                    # Update weights with stabilized denominator\n",
    "                    self.weights[i] -= learning_rate * w_momentum / np.sqrt(w_hist_corr + self.epsilon)\n",
    "                    self.biases[i] -= learning_rate * b_momentum / np.sqrt(b_hist_corr + self.epsilon)\n",
    "        else:  # NAG optimizer\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                # Compute gradients with weight decay\n",
    "                dw = (np.dot(neuron_outputs[i].T, delta) / batch_size) + weight_decay * self.weights[i]\n",
    "                db = (np.sum(delta, axis=0, keepdims=True) / batch_size) + weight_decay * self.biases[i]\n",
    "                if i > 0:\n",
    "                    # Propagate delta with NAG-adjusted weights\n",
    "                    delta = np.dot(delta, (self.weights[i] - self.beta_1 * self.weights_velocity[i]).T) * \\\n",
    "                            self.get_activation_derivative(neuron_outputs[i])\n",
    "                # Update velocities and parameters\n",
    "                self.weights_velocity[i] = self.beta_1 * self.weights_velocity[i] + dw\n",
    "                self.bias_velocity[i] = self.beta_1 * self.bias_velocity[i] + db\n",
    "                self.weights[i] -= learning_rate * self.weights_velocity[i]\n",
    "                self.biases[i] -= learning_rate * self.bias_velocity[i]\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=50, learning_rate=0.001, batch_size=32,\n",
    "              optimizer=\"nag\", beta_1=0.900, beta_2=0.999, loss_function='categorical_cross_entropy',\n",
    "              weight_decay=0):\n",
    "        \"\"\"\n",
    "        Train the neural network with mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            x_train (np.ndarray): Training input data.\n",
    "            y_train (np.ndarray): Training labels (one-hot encoded).\n",
    "            x_val (np.ndarray): Validation input data.\n",
    "            y_val (np.ndarray): Validation labels (one-hot encoded).\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for updates.\n",
    "            batch_size (int): Size of each mini-batch.\n",
    "            optimizer (str): Optimizer type.\n",
    "            beta_1 (float): Momentum/RMSProp/Adam parameter.\n",
    "            beta_2 (float): Adam/Nadam parameter.\n",
    "            loss_function (str): Loss function type.\n",
    "            weight_decay (float): L2 regularization parameter.\n",
    "        \"\"\"\n",
    "        # Set optimizer parameters\n",
    "        self.beta_1 = beta_1  # Update instance beta_1\n",
    "        self.beta_2 = beta_2  # Update instance beta_2\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data for each epoch\n",
    "            indices = np.random.permutation(x_train.shape[0])\n",
    "            x_train_permuted = x_train[indices]\n",
    "            y_train_permuted = y_train[indices]\n",
    "            total_loss = 0  # Accumulate loss over batches\n",
    "            batch_num = 0  # Count batches for averaging\n",
    "            # Mini-batch training\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                batch_x = x_train_permuted[i:i + batch_size]  # Extract batch input\n",
    "                batch_y = y_train_permuted[i:i + batch_size]  # Extract batch labels\n",
    "                neuron_outputs = self.forward_pass(batch_x, optimizer)  # Forward pass\n",
    "                # Compute L2 regularization term\n",
    "                l2_norm_params = sum(np.sum(w ** 2) for w in self.weights) + sum(np.sum(b ** 2) for b in self.biases)\n",
    "                # Compute loss with epsilon for stability\n",
    "                if loss_function.lower() == 'categorical_cross_entropy':\n",
    "                    loss = (-np.mean(np.sum(batch_y * np.log(neuron_outputs[-1] + self.epsilon), axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                else:  # Mean squared error\n",
    "                    loss = (np.mean(np.sum((batch_y - neuron_outputs[-1]) ** 2, axis=1)) +\n",
    "                            (weight_decay / 2) * l2_norm_params)\n",
    "                total_loss += loss  # Add to total loss\n",
    "                batch_num += 1  # Increment batch counter\n",
    "                # Backward pass to update weights\n",
    "                self.backward_pass(batch_x, batch_y, neuron_outputs, learning_rate, batch_num, optimizer,\n",
    "                                  loss_function, weight_decay)\n",
    "            # Compute average loss for the epoch\n",
    "            average_loss = total_loss / batch_num\n",
    "            # Evaluate on validation set\n",
    "            validation_predictions = self.predict(x_val)\n",
    "            validation_accuracy = np.mean(validation_predictions == np.argmax(y_val, axis=1))\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({'epoch': epoch, 'train_loss': average_loss, 'val_accuracy': validation_accuracy})\n",
    "            # Print progress\n",
    "            print(f\"epoch: {epoch}, train_loss: {average_loss:.4f}, val_accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted class indices.\n",
    "        \"\"\"\n",
    "        neuron_outputs = self.forward_pass(x)  # Forward pass\n",
    "        return np.argmax(neuron_outputs[-1], axis=1)  # Return class with highest probability\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up command-line argument parser\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network with backpropagation from scratch.')\n",
    "    \n",
    "    # Dataset choice: mnist or fashion_mnist\n",
    "    parser.add_argument('--dataset', type=str, default='fashion_mnist', choices=['mnist', 'fashion_mnist'],\n",
    "                        help='Dataset to use (mnist or fashion_mnist)')\n",
    "    # Hidden layer sizes as a list (e.g., --hidden_sizes 128 64 for two layers)\n",
    "    parser.add_argument('--hidden_sizes', type=int, nargs='+', default=[128, 64],\n",
    "                        help='List of hidden layer sizes (e.g., 128 64)')\n",
    "    # Activation function choice\n",
    "    parser.add_argument('--activation_function', type=str, default='relu',\n",
    "                        choices=['sigmoid', 'relu', 'tanh', 'linear'],\n",
    "                        help='Activation function for hidden layers')\n",
    "    # Learning rate\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                        help='Learning rate for optimization')\n",
    "    # Number of epochs\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help='Number of training epochs')\n",
    "    # Batch size\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Size of each mini-batch')\n",
    "    # Optimizer choice\n",
    "    parser.add_argument('--optimizer', type=str, default='rms_prop',\n",
    "                        choices=['sgd', 'momentum sgd', 'nag', 'rms_prop', 'adam', 'nadam'],\n",
    "                        help='Optimizer type')\n",
    "    # Beta1 for optimizers\n",
    "    parser.add_argument('--beta_1', type=float, default=0.900,\n",
    "                        help='Beta1 parameter for momentum/RMSProp/Adam')\n",
    "    # Beta2 for Adam/Nadam\n",
    "    parser.add_argument('--beta_2', type=float, default=0.999,\n",
    "                        help='Beta2 parameter for Adam/Nadam')\n",
    "    # Loss function choice\n",
    "    parser.add_argument('--loss_function', type=str, default='categorical_cross_entropy',\n",
    "                        choices=['categorical_cross_entropy', 'mean_squared_error'],\n",
    "                        help='Loss function type')\n",
    "    # Weight decay (L2 regularization)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0,\n",
    "                        help='Weight decay (L2 regularization) parameter')\n",
    "    # Epsilon for numerical stability\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                        help='Epsilon value for numerical stability')\n",
    "    # Parse arguments from command line\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Convert parsed arguments to a dictionary for wandb config\n",
    "    config = vars(args)\n",
    "    # Initialize wandb with the config; for sweeps, wandb will override specified hyperparameters\n",
    "    wandb.init(project='backprop_scratch', config=config)\n",
    "\n",
    "    def train():\n",
    "        \"\"\"Train the model using hyperparameters from wandb.config.\"\"\"\n",
    "        config = wandb.config  # Access hyperparameters (from command line or sweep)\n",
    "        # Load dataset based on config.dataset\n",
    "        if config.dataset == 'mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        elif config.dataset == 'fashion_mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {config.dataset}\")\n",
    "\n",
    "        # Preprocess data: flatten and normalize images, one-hot encode labels\n",
    "        train_images = train_images.reshape(-1, 784) / 255.0  # Flatten to 784, normalize to [0, 1]\n",
    "        test_images = test_images.reshape(-1, 784) / 255.0  # Same for test set\n",
    "        train_labels = np.eye(10)[train_labels]  # One-hot encode training labels\n",
    "        test_labels = np.eye(10)[test_labels]  # One-hot encode test labels\n",
    "\n",
    "        # Split training data into train and validation sets (50000 train, 10000 val)\n",
    "        indices = np.arange(train_images.shape[0])\n",
    "        np.random.shuffle(indices)  # Randomize indices\n",
    "        train_size = 50000  # Fixed split as in original code\n",
    "        train_x = train_images[indices[:train_size]]  # Training input\n",
    "        train_y = train_labels[indices[:train_size]]  # Training labels\n",
    "        val_x = train_images[indices[train_size:]]  # Validation input\n",
    "        val_y = train_labels[indices[train_size:]]  # Validation labels\n",
    "\n",
    "        # Construct layer_size: input (784), hidden layers, output (10)\n",
    "        layer_size = [784] + config.hidden_sizes + [10]  # Flexible hidden layers from config\n",
    "        # Create model instance with specified parameters\n",
    "        model = backprop_from_scratch(layer_size, config.activation_function, config.epsilon)\n",
    "        # Train the model with all hyperparameters from config\n",
    "        model.train(train_x, train_y, val_x, val_y, config.epochs, config.learning_rate, config.batch_size,\n",
    "                    config.optimizer, config.beta_1, config.beta_2, config.loss_function, config.weight_decay)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_predictions = model.predict(test_images)  # Get predictions\n",
    "        test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))  # Compute accuracy\n",
    "        wandb.log({'test_accuracy': test_accuracy})  # Log test accuracy to wandb\n",
    "        print(f\"test_accuracy: {test_accuracy:.4f}\")  # Print test accuracy\n",
    "\n",
    "    # Execute training\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09635c-ad96-4abb-a2e4-5c71e6f9279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train a neural network with backpropagation from scratch.')\n",
    "\n",
    "    # Weights & Biases arguments\n",
    "    parser.add_argument('-wp', '--wandb_project', type=str, default='myprojectname', help='Project name for Weights & Biases')\n",
    "    parser.add_argument('-we', '--wandb_entity', type=str, default='myname', help='Wandb Entity for tracking experiments')\n",
    "    \n",
    "    # Dataset and training arguments\n",
    "    parser.add_argument('-d', '--dataset', type=str, default='fashion_mnist', choices=['mnist', 'fashion_mnist'], help='Dataset to use')\n",
    "    parser.add_argument('-e', '--epochs', type=int, default=1, help='Number of training epochs')\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=4, help='Batch size')\n",
    "    parser.add_argument('-l', '--loss', type=str, default='cross_entropy', choices=['mean_squared_error', 'cross_entropy'], help='Loss function')\n",
    "    parser.add_argument('-o', '--optimizer', type=str, default='sgd', choices=['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam'], help='Optimizer')\n",
    "    parser.add_argument('-lr', '--learning_rate', type=float, default=0.1, help='Learning rate')\n",
    "    \n",
    "    # Optimizer-specific parameters\n",
    "    parser.add_argument('-m', '--momentum', type=float, default=0.5, help='Momentum for momentum-based optimizers')\n",
    "    parser.add_argument('-beta', '--beta', type=float, default=0.5, help='Beta for rmsprop optimizer')\n",
    "    parser.add_argument('-beta1', '--beta1', type=float, default=0.5, help='Beta1 for adam and nadam')\n",
    "    parser.add_argument('-beta2', '--beta2', type=float, default=0.5, help='Beta2 for adam and nadam')\n",
    "    parser.add_argument('-eps', '--epsilon', type=float, default=0.000001, help='Epsilon for numerical stability')\n",
    "    parser.add_argument('-w_d', '--weight_decay', type=float, default=0.0, help='Weight decay (L2 regularization)')\n",
    "    \n",
    "    # Neural network architecture\n",
    "    parser.add_argument('-w_i', '--weight_init', type=str, default='random', choices=['random', 'Xavier'], help='Weight initialization method')\n",
    "    parser.add_argument('-nhl', '--num_layers', type=int, default=1, help='Number of hidden layers')\n",
    "    parser.add_argument('-sz', '--hidden_size', type=int, default=4, help='Number of neurons in a hidden layer')\n",
    "    parser.add_argument('-a', '--activation', type=str, default='sigmoid', choices=['identity', 'sigmoid', 'tanh', 'ReLU'], help='Activation function')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Convert parsed arguments to a dictionary\n",
    "    config = vars(args)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project=config['wandb_project'], entity=config['wandb_entity'], config=config)\n",
    "\n",
    "    def train():\n",
    "        config = wandb.config  # Access hyperparameters\n",
    "        \n",
    "        # Load dataset\n",
    "        if config.dataset == 'mnist':\n",
    "            (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "        else:\n",
    "            (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_images = train_images.reshape(-1, 784) / 255.0\n",
    "        test_images = test_images.reshape(-1, 784) / 255.0\n",
    "        train_labels = np.eye(10)[train_labels]\n",
    "        test_labels = np.eye(10)[test_labels]\n",
    "        \n",
    "        # Split into train/validation\n",
    "        indices = np.arange(train_images.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        train_size = 50000\n",
    "        train_x = train_images[indices[:train_size]]\n",
    "        train_y = train_labels[indices[:train_size]]\n",
    "        val_x = train_images[indices[train_size:]]\n",
    "        val_y = train_labels[indices[train_size:]]\n",
    "        \n",
    "        # Construct layer sizes\n",
    "        layer_size = [784] + [config.hidden_size] * config.num_layers + [10]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = backprop_from_scratch(layer_size, config.activation, config.epsilon)\n",
    "        model.train(train_x, train_y, val_x, val_y, config.epochs, config.learning_rate, config.batch_size,\n",
    "                    config.optimizer, config.momentum, config.beta, config.beta1, config.beta2,\n",
    "                    config.loss, config.weight_decay)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions = model.predict(test_images)\n",
    "        test_accuracy = np.mean(test_predictions == np.argmax(test_labels, axis=1))\n",
    "        wandb.log({'test_accuracy': test_accuracy})\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
